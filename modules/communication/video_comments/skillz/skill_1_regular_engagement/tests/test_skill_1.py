"""
Test Suite for Skill 1: Regular Engagement
Phase 3O-3R Sprint 4 - Verify skill extraction and 3-tier strategy

WSP References:
- WSP 5 (Test Coverage)
- WSP 6 (Test Audit)
- WSP 96 (WRE Skills): Skill execution pattern
"""

import sys
import io
from pathlib import Path

# Fix Windows console encoding for emojis (WSP 90: UTF-8 enforcement)
if sys.platform.startswith('win'):
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except (OSError, ValueError):
        pass

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Direct import from executor module
from modules.communication.video_comments.skillz.skill_1_regular_engagement.executor import (
    RegularEngagementSkill,
    SkillContext
)


def test_llm_reply_strategy():
    """Test 1: LLM reply takes priority (like Skill 0's maga_response)"""
    print("\n=== TEST 1: LLM Contextual Reply Strategy ===")

    skill = RegularEngagementSkill()

    # Context WITH pre-generated LLM reply
    context = SkillContext(
        user_id="regular_user_001",
        username="RegularUser",
        comment_text="Bro got the dance moves! üï∫",
        classification="REGULAR",
        confidence=0.5,
        llm_reply="Haha yeah! The choreography is fire! üî•"  # Pre-generated by caller
    )

    result = skill.execute(context)

    print(f"[TEST] Strategy: {result['strategy']}")
    print(f"[TEST] Reply: {result['reply_text']}")
    print(f"[TEST] Confidence: {result['confidence']}")

    if result['strategy'] == 'llm_contextual' and result['reply_text'] == context.llm_reply:
        print("[OK] LLM contextual reply strategy works!")
        return True
    else:
        print("[FAIL] Expected llm_contextual strategy")
        return False


def test_banter_engine_fallback():
    """Test 2: BanterEngine used when LLM unavailable"""
    print("\n=== TEST 2: BanterEngine Fallback ===")

    skill = RegularEngagementSkill()

    # Context WITHOUT LLM reply
    context = SkillContext(
        user_id="regular_user_002",
        username="AnotherUser",
        comment_text="Great content!",
        classification="REGULAR",
        confidence=0.5,
        llm_reply=None,  # No LLM reply
        theme="default"
    )

    result = skill.execute(context)

    print(f"[TEST] Strategy: {result['strategy']}")
    print(f"[TEST] Reply: {result['reply_text']}")
    print(f"[TEST] Confidence: {result['confidence']}")

    # BanterEngine might not be available (expected), so check fallback to templates
    if result['strategy'] in ['banter_engine', 'template_regular']:
        print(f"[OK] Fallback strategy works! (used: {result['strategy']})")
        return True
    else:
        print(f"[FAIL] Unexpected strategy: {result['strategy']}")
        return False


def test_template_fallback_regular():
    """Test 3: Template fallback for regular users"""
    print("\n=== TEST 3: Template Fallback (Regular) ===")

    skill = RegularEngagementSkill()

    # Mark BanterEngine as unavailable to force template fallback
    skill._banter_engine = False

    context = SkillContext(
        user_id="regular_user_003",
        username="TemplateUser",
        comment_text="Nice video!",
        classification="REGULAR",
        confidence=0.5,
        llm_reply=None,
        is_subscriber=False
    )

    result = skill.execute(context)

    print(f"[TEST] Strategy: {result['strategy']}")
    print(f"[TEST] Reply: {result['reply_text']}")
    print(f"[TEST] Confidence: {result['confidence']}")

    # Verify template strategy used
    if (result['strategy'] == 'template_regular' and
        result['reply_text'] in skill.REGULAR_RESPONSES):
        print("[OK] Template fallback works for regular users!")
        return True
    else:
        print("[FAIL] Expected template_regular strategy")
        return False


def test_template_fallback_subscriber():
    """Test 4: Template fallback for subscribers"""
    print("\n=== TEST 4: Template Fallback (Subscriber) ===")

    skill = RegularEngagementSkill()

    # Mark BanterEngine as unavailable
    skill._banter_engine = False

    context = SkillContext(
        user_id="subscriber_001",
        username="SubscriberUser",
        comment_text="Love your content!",
        classification="REGULAR",  # Subscribers now classified as REGULAR in new system
        confidence=0.5,
        llm_reply=None,
        is_subscriber=True  # But flag still indicates subscriber
    )

    result = skill.execute(context)

    print(f"[TEST] Strategy: {result['strategy']}")
    print(f"[TEST] Reply: {result['reply_text']}")
    print(f"[TEST] Confidence: {result['confidence']}")

    # Verify subscriber templates used
    if (result['strategy'] == 'template_subscriber' and
        result['reply_text'] in skill.SUBSCRIBER_RESPONSES):
        print("[OK] Template fallback works for subscribers!")
        return True
    else:
        print("[FAIL] Expected template_subscriber strategy")
        return False


def test_response_variation():
    """Test 5: Verify responses vary (not always same template)"""
    print("\n=== TEST 5: Response Variation ===")

    skill = RegularEngagementSkill()
    skill._banter_engine = False  # Force template fallback

    context = SkillContext(
        user_id="regular_user_004",
        username="VariationUser",
        comment_text="Test",
        classification="REGULAR",
        confidence=0.5,
        llm_reply=None,
        is_subscriber=False
    )

    # Execute 10 times, collect unique responses
    responses = set()
    for i in range(10):
        result = skill.execute(context)
        responses.add(result['reply_text'])

    print(f"[TEST] Unique responses: {len(responses)} out of 10 executions")

    if len(responses) > 1:
        print("[OK] Responses vary (randomized selection working)")
        return True
    else:
        print("[FAIL] All responses identical (random selection broken?)")
        return False


def test_context_validation():
    """Test 6: Verify skill handles minimal context"""
    print("\n=== TEST 6: Context Validation ===")

    skill = RegularEngagementSkill()
    skill._banter_engine = False

    # Minimal context (only required fields)
    context = SkillContext(
        user_id="regular_user_005",
        username="MinimalUser",
        comment_text="Hi",
        classification="REGULAR",
        confidence=0.5
    )

    try:
        result = skill.execute(context)
        print(f"[TEST] Strategy: {result['strategy']}")
        print(f"[TEST] Reply: {result['reply_text'][:50]}...")
        print("[OK] Skill handles minimal context!")
        return True
    except Exception as e:
        print(f"[FAIL] Skill crashed with minimal context: {e}")
        return False


def test_strategy_priority():
    """Test 7: Verify strategy priority (LLM > Banter > Templates)"""
    print("\n=== TEST 7: Strategy Priority ===")

    skill = RegularEngagementSkill()

    # Test with LLM reply provided (should use LLM, not fallback)
    context_with_llm = SkillContext(
        user_id="regular_user_006",
        username="PriorityUser",
        comment_text="Test",
        classification="REGULAR",
        confidence=0.5,
        llm_reply="Custom LLM response"
    )

    result = skill.execute(context_with_llm)

    if result['strategy'] != 'llm_contextual':
        print(f"[FAIL] LLM reply provided but strategy was {result['strategy']}")
        return False

    print("[TEST] LLM strategy priority: ‚úì")

    # Test without LLM (should fallback)
    skill._banter_engine = False  # Disable BanterEngine
    context_without_llm = SkillContext(
        user_id="regular_user_007",
        username="FallbackUser",
        comment_text="Test",
        classification="REGULAR",
        confidence=0.5,
        llm_reply=None
    )

    result = skill.execute(context_without_llm)

    if result['strategy'] != 'template_regular':
        print(f"[FAIL] Expected template fallback, got {result['strategy']}")
        return False

    print("[TEST] Template fallback priority: ‚úì")
    print("[OK] Strategy priority chain works!")
    return True


def run_all_tests():
    """Run all Skill 1 tests"""
    print("=" * 60)
    print("SKILL 1: REGULAR ENGAGEMENT TESTS - Sprint 4")
    print("=" * 60)

    tests = [
        ("LLM Contextual Reply Strategy", test_llm_reply_strategy),
        ("BanterEngine Fallback", test_banter_engine_fallback),
        ("Template Fallback (Regular)", test_template_fallback_regular),
        ("Template Fallback (Subscriber)", test_template_fallback_subscriber),
        ("Response Variation", test_response_variation),
        ("Context Validation", test_context_validation),
        ("Strategy Priority", test_strategy_priority),
    ]

    passed = 0
    failed = 0

    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"[ERROR] {test_name} crashed: {e}")
            import traceback
            traceback.print_exc()
            failed += 1

    print("\n" + "=" * 60)
    print(f"RESULTS: {passed} passed, {failed} failed")
    print("=" * 60)

    if failed == 0:
        print("\n‚úÖ ALL TESTS PASSED - Skill 1 ready for integration!")
        return True
    else:
        print(f"\n‚ùå {failed} TEST(S) FAILED - Review failures above")
        return False


if __name__ == "__main__":
    import sys
    success = run_all_tests()
    sys.exit(0 if success else 1)
