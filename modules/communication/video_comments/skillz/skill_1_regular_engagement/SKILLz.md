# Skill 1: Regular Engagement

**Phase:** 3O-3R Sprint 4
**Classification:** 1âœ‹ (REGULAR)
**Primary Agent:** 0102 (rule-based) + optional LLM/BanterEngine
**Intent Type:** GENERATION (contextual replies)
**Promotion State:** prototype â†’ staged (Sprint 4 complete, awaiting Sprint 5 router integration)

## Overview

Generates contextual, engaging replies for regular users (classified as 1âœ‹), including both regular viewers and subscribers in the new 0/1/2 classification system.

**Pattern Source:** Extracted from `intelligent_reply_generator.py` lines 1039-1056

## WSP Compliance

- **WSP 96 (WRE Skills)**: Skill separation pattern - extracted from monolithic generator
- **WSP 77 (Agent Coordination)**: Classification-based routing (Classifier â†’ Skill)
- **WSP 84 (Code Reuse)**: Reuses BanterEngine, follows Skill 0 pattern (pre-generated LLM replies)

## Architecture

```
CommenterClassifier â†’ 1âœ‹ (REGULAR)
         â†“
   RegularEngagementSkill.execute()
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Strategy 1 (if available)    â”‚ Strategy 2 (if available)          â”‚
   â”‚ LLM Contextual Reply         â”‚ BanterEngine                       â”‚
   â”‚ Pre-generated by caller      â”‚ Theme-based banter                 â”‚
   â”‚ (context.llm_reply)          â”‚ Lazy-loaded                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Strategy 3 (fallback)                                           â”‚
   â”‚ Template Responses                                              â”‚
   â”‚ REGULAR_RESPONSES or SUBSCRIBER_RESPONSES                       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   Return unsignified text (caller adds âœŠâœ‹ðŸ–ï¸ signature)
```

## Response Strategies

### Strategy 1: LLM Contextual Reply (Primary)

**Source**: Pre-generated by caller (via Grok API or LM Studio)

Contextual, meaningful replies generated by LLM:

```
Examples:
- Comment: "Bro got the dance moves! ðŸ•º"
  Reply: "Haha yeah! The choreography is fire! ðŸ”¥"

- Comment: "This is exactly what I needed to hear today"
  Reply: "So glad this resonated with you! ðŸ’™"
```

**When used**: If `context.llm_reply` is provided (following Skill 0 pattern with `maga_response`)

**Pattern**: Like Skill 0 accepts pre-generated `maga_response`, Skill 1 accepts pre-generated `llm_reply`

### Strategy 2: BanterEngine (Secondary)

**Source**: `modules/ai_intelligence/banter_engine/src/banter_engine.py`

Theme-based banter responses:

```
Examples:
- "Your emoji game is strong! ðŸ’ª"
- "Love the energy in the comments! ðŸ”¥"
```

**When used**: If `context.llm_reply` is None AND BanterEngine available

**Lazy-loading**: BanterEngine initialized on first use, marked as unavailable if load fails

### Strategy 3: Template Responses (Tertiary)

**Source**: 5 regular templates + 5 subscriber templates

**REGULAR_RESPONSES** (lines 235-241):
```
- "Thanks for watching! ðŸŽŒ"
- "Great point! ðŸ‘"
- "Thanks for the comment! ðŸ˜Š"
- "Appreciate the feedback! ðŸ™"
- "Thanks for joining! ðŸŒŸ"
```

**SUBSCRIBER_RESPONSES** (lines 227-233):
```
- "Thanks for the support! ðŸŽŒ"
- "Arigatou gozaimasu! ðŸ‡¯ðŸ‡µ"
- "Appreciate you! ðŸ’™"
- "You're awesome! â­"
- "Thanks for being part of the community! ðŸ™Œ"
```

**When used**: If LLM and BanterEngine both unavailable, or as ultimate fallback

## Usage

### Standalone Execution

```python
from modules.communication.video_comments.skills.skill_1_regular_engagement import (
    RegularEngagementSkill,
    SkillContext
)

skill = RegularEngagementSkill()

# With pre-generated LLM reply
context = SkillContext(
    user_id="regular_user_id",
    username="RegularUser",
    comment_text="Bro got the dance moves! ðŸ•º",
    classification="REGULAR",
    confidence=0.5,
    llm_reply="Haha yeah! The choreography is fire! ðŸ”¥"  # From Grok/LM Studio
)

result = skill.execute(context)
# Returns: {
#   'reply_text': 'Haha yeah! The choreography is fire! ðŸ”¥',
#   'strategy': 'llm_contextual',
#   'confidence': 0.9
# }

# Without LLM reply (BanterEngine or template fallback)
context_fallback = SkillContext(
    user_id="regular_user_id",
    username="RegularUser",
    comment_text="Great content!",
    classification="REGULAR",
    confidence=0.5,
    llm_reply=None,  # No LLM reply
    theme="default"
)

result = skill.execute(context_fallback)
# Returns template or banter response
```

### Integration with IntelligentReplyGenerator (Sprint 5)

**Current (Sprint 4)**: Skill exists alongside monolithic code (no integration yet)

**Future (Sprint 5)**: Replace lines 1039-1056 with skill router:

```python
# intelligent_reply_generator.py (Sprint 5 integration)
else:  # REGULAR USER
    from modules.communication.video_comments.skills.skill_1_regular_engagement import RegularEngagementSkill

    # Generate LLM reply if available (like current code does)
    llm_reply = self._generate_contextual_reply(comment_text, author_name, author_channel_id)

    skill = RegularEngagementSkill()
    result = skill.execute(SkillContext(
        user_id=author_channel_id,
        username=author_name,
        comment_text=comment_text,
        classification='REGULAR',
        confidence=profile.confidence or 0.5,
        llm_reply=llm_reply,  # Pass pre-generated reply (can be None)
        theme=theme,
        is_subscriber=profile.is_subscriber
    ))

    return self._add_0102_signature(result['reply_text'])
```

## Dependencies

### Required
- Python 3.12+
- `logging`, `random`, `dataclasses` (stdlib)

### Optional
- **BanterEngine**: Provides theme-based banter responses
  - Location: `modules/ai_intelligence/banter_engine/src/banter_engine.py`
  - Fallback: Uses template responses if unavailable
  - Lazy-loaded on first use

### Caller Responsibilities
- **LLM Reply Generation**: Caller generates `context.llm_reply` via Grok or LM Studio
  - `_generate_contextual_reply()` method in IntelligentReplyGenerator
  - Can be None (skill will fallback to BanterEngine or templates)

## Testing

### Run Tests

```bash
cd O:\Foundups-Agent
python modules/communication/video_comments/skills/skill_1_regular_engagement/tests/test_skill_1.py
```

### Test Coverage

âœ… **7/7 tests passing** (Sprint 4)

1. **LLM Contextual Reply Strategy**: Verifies LLM reply takes priority
2. **BanterEngine Fallback**: Confirms BanterEngine used when LLM unavailable
3. **Template Fallback (Regular)**: Validates REGULAR_RESPONSES fallback
4. **Template Fallback (Subscriber)**: Validates SUBSCRIBER_RESPONSES fallback
5. **Response Variation**: Confirms randomized template selection (4 unique responses)
6. **Context Validation**: Handles minimal required fields
7. **Strategy Priority**: Verifies LLM > BanterEngine > Templates chain

## Performance

- **Execution time**: <5ms (LLM pre-generated, local logic only)
- **BanterEngine load**: ~50ms (one-time lazy load)
- **Memory footprint**: ~5KB (10 template strings + lazy dependencies)
- **Scalability**: O(1) - constant time selection

## Future Enhancements

### Sprint 6+ (LLM Integration Enhancement)
- Embed `_generate_contextual_reply()` directly in skill (self-contained LLM calls)
- Add semantic variation prompts (ANTI-REGURGITATION Phase 3N)
- Add duplicate detection (CommenterHistoryStore integration)
- Add personalization context (comment history)

### Sprint 5 (Router Integration)
- Integrate into skill router (replace monolithic code)
- Add history-aware duplicate detection
- Metric tracking per skill execution

### Post-Sprint 5 (Learning Layer)
- A/B test different reply styles (LLM vs Banter vs Templates)
- Learn which responses get the most engagement
- Pattern memory integration (WSP 60)
- Multi-language support

## Changelog

### Sprint 4 (2025-12-19) - Initial Extraction
- âœ… Extracted from `intelligent_reply_generator.py` lines 1039-1056
- âœ… Created standalone `RegularEngagementSkill` class
- âœ… Implemented 3-tier strategy (LLM â†’ BanterEngine â†’ Templates)
- âœ… Followed Skill 0 pattern (pre-generated LLM replies via context)
- âœ… Lazy-loaded BanterEngine dependency
- âœ… Added 7 unit tests (100% passing)
- âœ… Documented skill architecture and integration
- ðŸ“‹ Awaiting Sprint 5 router integration

### Design Decisions

**Why pre-generated LLM replies instead of embedded?**
- Follows Skill 0 pattern (`maga_response` pre-generated by GrokGreetingGenerator)
- Keeps skill lightweight and focused
- Allows caller to control LLM strategy (Grok vs LM Studio)
- Future: Can embed LLM logic in Sprint 6+ for full self-containment

**Why lazy-load BanterEngine?**
- Follows Skill 2 pattern (lazy-loaded ChatRulesDB)
- Graceful degradation if BanterEngine unavailable
- Minimal overhead for template-only fallback

**Why separate REGULAR vs SUBSCRIBER templates?**
- Backward compatibility with old classification system
- Subscriber flag (`is_subscriber`) still exists in context
- Future: Can merge into single template set after migration complete

## Related Files

- **Executor**: `executor.py` (RegularEngagementSkill class)
- **Tests**: `tests/test_skill_1.py` (7 unit tests)
- **Integration**: Replaces `intelligent_reply_generator.py` lines 1039-1056 (Sprint 5)
- **LLM Source**: `intelligent_reply_generator.py` `_generate_contextual_reply()` method
- **Banter Source**: `modules/ai_intelligence/banter_engine/src/banter_engine.py`

## License

WSP Compliant Module - Part of FoundUps Agent System
