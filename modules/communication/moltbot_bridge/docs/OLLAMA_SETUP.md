# Moltbot Local Models (Ollama) Configuration

> [!TIP]
> Use Ollama for zero-cost local inference without API keys.

## Ollama Setup

### 1. Install Ollama

**Windows** (native, no WSL needed for Ollama itself):
```powershell
winget install Ollama.Ollama
```

Or download from [ollama.ai](https://ollama.ai)

### 2. Pull a Model

```bash
# Recommended for coding tasks
ollama pull qwen2.5-coder:32b

# Or for general use
ollama pull llama3.1:70b
```

### 3. Start Ollama Server

```bash
ollama serve
# Runs on http://localhost:11434
```

---

## Moltbot Configuration for Ollama

Add to `~/.clawdbot/moltbot.json`:

```json
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "ollama/qwen2.5-coder:32b"
      },
      "models": {
        "ollama/qwen2.5-coder:32b": { "alias": "Qwen Coder" }
      }
    }
  },
  "models": {
    "mode": "merge",
    "providers": {
      "ollama": {
        "baseUrl": "http://localhost:11434/v1",
        "apiKey": "ollama",
        "api": "openai-completions",
        "models": [
          {
            "id": "qwen2.5-coder:32b",
            "name": "Qwen 2.5 Coder 32B",
            "reasoning": false,
            "input": ["text"],
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
            "contextWindow": 32768,
            "maxTokens": 8192
          }
        ]
      }
    }
  }
}
```

---

## Hybrid: Ollama + Cloud Fallback

Use Ollama locally, fall back to Claude for complex tasks:

```json
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "ollama/qwen2.5-coder:32b",
        "fallback": "anthropic/claude-sonnet-4"
      },
      "models": {
        "ollama/qwen2.5-coder:32b": { "alias": "Local" },
        "anthropic/claude-sonnet-4": { "alias": "Cloud" }
      }
    }
  }
}
```

---

## WSL + Ollama (Windows)

If running Moltbot in WSL2, Ollama on Windows is accessible via:
```
http://host.docker.internal:11434/v1
```

Or get Windows IP:
```powershell
(Get-NetIPAddress -AddressFamily IPv4 -InterfaceAlias "Ethernet").IPAddress
```

Then use that IP in WSL config:
```json
{
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://192.168.x.x:11434/v1"
      }
    }
  }
}
```

---

## Recommended Models for 012 Digital Twin

| Model | Size | Use Case |
|-------|------|----------|
| `qwen2.5-coder:32b` | ~20GB | Code generation, WSP development |
| `llama3.1:70b` | ~40GB | General reasoning |
| `deepseek-coder:33b` | ~20GB | Alternative for coding |
| `gemma2:27b` | ~16GB | Lighter weight option |

Choose based on your VRAM. 32B models need ~24GB VRAM or CPU offloading.
