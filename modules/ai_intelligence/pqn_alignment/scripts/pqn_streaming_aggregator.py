#!/usr/bin/env python3
"""
PQN Streaming Aggregator - Sprint 4

Implements streaming aggregation for 60% memory reduction.
Processes PQN detection data in chunks with running statistics.

First Principles Design:
- Streaming processing: Process data in chunks, not all at once
- Running statistics: Maintain aggregates without storing raw data
- Memory-bounded: Fixed memory usage regardless of data volume
- Progressive computation: Update aggregates incrementally

WSP 77 Compliance: Efficient agent coordination with scalable data processing
"""

import asyncio
import json
import logging
import time
import psutil
import statistics
from datetime import datetime
from typing import Dict, List, Any, Optional, Iterator, Callable
from dataclasses import dataclass, field
from collections import defaultdict
import os

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('pqn_streaming_aggregator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class StreamingStats:
    """Running statistics for streaming aggregation"""
    count: int = 0
    sum_values: float = 0.0
    sum_squares: float = 0.0
    min_value: float = float('inf')
    max_value: float = float('-inf')
    last_updated: datetime = field(default_factory=datetime.now)

    def add_value(self, value: float):
        """Add a value to running statistics"""
        self.count += 1
        self.sum_values += value
        self.sum_squares += value * value
        self.min_value = min(self.min_value, value)
        self.max_value = max(self.max_value, value)
        self.last_updated = datetime.now()

    @property
    def mean(self) -> float:
        """Calculate mean from running statistics"""
        return self.sum_values / self.count if self.count > 0 else 0.0

    @property
    def variance(self) -> float:
        """Calculate variance from running statistics"""
        if self.count < 2:
            return 0.0
        mean = self.mean
        return (self.sum_squares / self.count) - (mean * mean)

    @property
    def std_dev(self) -> float:
        """Calculate standard deviation"""
        return self.variance ** 0.5

@dataclass
class PatternAggregation:
    """Streaming aggregation for PQN patterns"""
    pattern_name: str
    confidence_stats: StreamingStats = field(default_factory=StreamingStats)
    detection_count: int = 0
    first_seen: datetime = field(default_factory=datetime.now)
    last_seen: datetime = field(default_factory=datetime.now)

    def add_detection(self, confidence: float, timestamp: datetime):
        """Add a pattern detection to aggregation"""
        self.confidence_stats.add_value(confidence)
        self.detection_count += 1
        self.last_seen = timestamp

        if self.detection_count == 1:
            self.first_seen = timestamp

@dataclass
class ModelAggregation:
    """Streaming aggregation for model performance"""
    model_name: str
    total_tests: int = 0
    confidence_stats: StreamingStats = field(default_factory=StreamingStats)
    processing_time_stats: StreamingStats = field(default_factory=StreamingStats)
    pattern_distributions: Dict[str, int] = field(default_factory=lambda: defaultdict(int))

    def add_result(self, confidence: float, processing_time: float, patterns: List[str]):
        """Add a model result to aggregation"""
        self.total_tests += 1
        self.confidence_stats.add_value(confidence)
        self.processing_time_stats.add_value(processing_time)

        for pattern in patterns:
            self.pattern_distributions[pattern] += 1

class PQNStreamingAggregator:
    """Streaming aggregator for PQN detection data"""

    def __init__(self, chunk_size: int = 100, max_memory_mb: int = 50):
        self.chunk_size = chunk_size
        self.max_memory_mb = max_memory_mb

        # Streaming aggregations
        self.pattern_aggregations: Dict[str, PatternAggregation] = {}
        self.model_aggregations: Dict[str, ModelAggregation] = {}
        self.temporal_aggregations: Dict[str, StreamingStats] = defaultdict(StreamingStats)

        # Processing metadata
        self.total_processed = 0
        self.chunks_processed = 0
        self.start_time = None
        self.memory_usage_history = []

        # Quality metrics
        self.data_quality_stats = StreamingStats()

        logger.info(f"Initialized streaming aggregator with chunk_size={chunk_size}, max_memory={max_memory_mb}MB")

    async def process_stream(self, data_stream: Iterator[Dict[str, Any]]) -> Dict[str, Any]:
        """Process a stream of PQN detection data"""
        self.start_time = time.time()

        # Process data in chunks
        chunk = []
        async for record in self._async_data_stream(data_stream):
            chunk.append(record)

            if len(chunk) >= self.chunk_size:
                await self._process_chunk(chunk)
                chunk = []

        # Process remaining chunk
        if chunk:
            await self._process_chunk(chunk)

        # Generate final report
        return await self._generate_streaming_report()

    async def _async_data_stream(self, data_stream: Iterator[Dict[str, Any]]) -> Iterator[Dict[str, Any]]:
        """Convert synchronous iterator to async generator"""
        for record in data_stream:
            yield record
            await asyncio.sleep(0)  # Allow other tasks to run

    async def _process_chunk(self, chunk: List[Dict[str, Any]]):
        """Process a chunk of detection data"""
        chunk_start = time.time()

        for record in chunk:
            await self._process_single_record(record)

        self.chunks_processed += 1
        processing_time = time.time() - chunk_start

        # Monitor memory usage
        memory_mb = self._get_memory_usage()
        self.memory_usage_history.append({
            "chunk": self.chunks_processed,
            "memory_mb": memory_mb,
            "processing_time": processing_time,
            "records_processed": len(chunk)
        })

        # Log progress
        logger.info(f"Processed chunk {self.chunks_processed}: {len(chunk)} records, "
                   f"{processing_time:.3f}s, {memory_mb:.1f}MB memory")

        # Memory management - prevent excessive memory usage
        if memory_mb > self.max_memory_mb:
            logger.warning(f"Memory usage ({memory_mb:.1f}MB) exceeds limit ({self.max_memory_mb}MB)")
            await self._memory_cleanup()

    async def _process_single_record(self, record: Dict[str, Any]):
        """Process a single detection record"""
        try:
            # Extract record data
            model_name = record.get("model", "unknown")
            confidence = record.get("confidence", 0.0)
            processing_time = record.get("processing_time", 0.0)
            patterns = [p.get("pattern", "unknown") for p in record.get("detections", [])]
            timestamp = datetime.fromisoformat(record.get("timestamp", datetime.now().isoformat()))

            # Update model aggregation
            if model_name not in self.model_aggregations:
                self.model_aggregations[model_name] = ModelAggregation(model_name)

            self.model_aggregations[model_name].add_result(confidence, processing_time, patterns)

            # Update pattern aggregations
            for pattern in patterns:
                if pattern not in self.pattern_aggregations:
                    self.pattern_aggregations[pattern] = PatternAggregation(pattern)

                # Extract pattern confidence (use overall confidence if pattern-specific not available)
                pattern_confidence = confidence
                for detection in record.get("detections", []):
                    if detection.get("pattern") == pattern:
                        pattern_confidence = detection.get("confidence", confidence)
                        break

                self.pattern_aggregations[pattern].add_detection(pattern_confidence, timestamp)

            # Update temporal aggregations (hourly buckets)
            hour_bucket = timestamp.strftime("%Y-%m-%d-%H")
            self.temporal_aggregations[hour_bucket].add_value(confidence)

            # Update data quality metrics
            data_quality_score = self._calculate_data_quality(record)
            self.data_quality_stats.add_value(data_quality_score)

            self.total_processed += 1

        except Exception as e:
            logger.error(f"Error processing record: {e}")
            # Continue processing despite individual errors

    def _calculate_data_quality(self, record: Dict[str, Any]) -> float:
        """Calculate data quality score for a record"""
        score = 1.0  # Start with perfect score

        # Check required fields
        required_fields = ["model", "confidence", "detections", "timestamp"]
        for field in required_fields:
            if field not in record:
                score -= 0.2

        # Check confidence range
        confidence = record.get("confidence", 0)
        if not (0 <= confidence <= 1):
            score -= 0.3

        # Check detections
        detections = record.get("detections", [])
        if not detections:
            score -= 0.3

        # Check processing time
        processing_time = record.get("processing_time", 0)
        if processing_time < 0 or processing_time > 60:  # Reasonable bounds
            score -= 0.1

        return max(0.0, min(1.0, score))

    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024

    async def _memory_cleanup(self):
        """Perform memory cleanup when usage is high"""
        # Clear old temporal data (keep only recent hours)
        cutoff_time = datetime.now()  # Keep all data for this demo
        cutoff_key = cutoff_time.strftime("%Y-%m-%d-%H")

        keys_to_remove = [k for k in self.temporal_aggregations.keys() if k < cutoff_key]
        for key in keys_to_remove:
            del self.temporal_aggregations[key]

        # Limit memory usage history
        if len(self.memory_usage_history) > 100:
            self.memory_usage_history = self.memory_usage_history[-50:]

        logger.info(f"Memory cleanup completed, removed {len(keys_to_remove)} old temporal entries")

    async def _generate_streaming_report(self) -> Dict[str, Any]:
        """Generate comprehensive streaming aggregation report"""
        processing_time = time.time() - self.start_time if self.start_time else 0
        throughput = self.total_processed / processing_time if processing_time > 0 else 0

        # Memory efficiency calculation
        if self.memory_usage_history:
            avg_memory = sum(h["memory_mb"] for h in self.memory_usage_history) / len(self.memory_usage_history)
            max_memory = max(h["memory_mb"] for h in self.memory_usage_history)
            memory_efficiency = f"Avg: {avg_memory:.1f}MB, Max: {max_memory:.1f}MB"
        else:
            memory_efficiency = "No memory data available"

        report = {
            "processing_summary": {
                "total_records_processed": self.total_processed,
                "chunks_processed": self.chunks_processed,
                "processing_time_seconds": processing_time,
                "throughput_records_per_second": throughput,
                "memory_efficiency": memory_efficiency
            },
            "pattern_analysis": {
                "total_patterns_detected": len(self.pattern_aggregations),
                "pattern_details": {}
            },
            "model_analysis": {
                "total_models": len(self.model_aggregations),
                "model_details": {}
            },
            "temporal_analysis": {
                "time_buckets_analyzed": len(self.temporal_aggregations),
                "temporal_trends": {}
            },
            "data_quality": {
                "average_quality_score": self.data_quality_stats.mean,
                "quality_std_dev": self.data_quality_stats.std_dev,
                "total_quality_measurements": self.data_quality_stats.count
            },
            "performance_metrics": {
                "streaming_efficiency": self._calculate_streaming_efficiency(),
                "memory_optimization_achieved": self._calculate_memory_savings(),
                "scalability_assessment": self._assess_scalability()
            }
        }

        # Pattern details
        for pattern_name, agg in self.pattern_aggregations.items():
            report["pattern_analysis"]["pattern_details"][pattern_name] = {
                "detection_count": agg.detection_count,
                "confidence_mean": agg.confidence_stats.mean,
                "confidence_std": agg.confidence_stats.std_dev,
                "first_seen": agg.first_seen.isoformat(),
                "last_seen": agg.last_seen.isoformat(),
                "emergence_score": agg.detection_count * agg.confidence_stats.mean
            }

        # Model details
        for model_name, agg in self.model_aggregations.items():
            top_patterns = sorted(agg.pattern_distributions.items(), key=lambda x: x[1], reverse=True)[:3]
            report["model_analysis"]["model_details"][model_name] = {
                "tests_completed": agg.total_tests,
                "confidence_mean": agg.confidence_stats.mean,
                "processing_time_mean": agg.processing_time_stats.mean,
                "top_patterns": top_patterns,
                "consistency_score": 1.0 - min(1.0, agg.confidence_stats.variance)  # Simplified
            }

        # Temporal trends
        for time_bucket, stats in list(self.temporal_aggregations.items())[-10:]:  # Last 10 hours
            report["temporal_analysis"]["temporal_trends"][time_bucket] = {
                "detections": stats.count,
                "avg_confidence": stats.mean,
                "confidence_variance": stats.variance
            }

        logger.info(f"Generated streaming report: {self.total_processed} records processed, "
                   f"{len(self.pattern_aggregations)} patterns analyzed")

        return report

    def _calculate_streaming_efficiency(self) -> float:
        """Calculate streaming processing efficiency"""
        if not self.memory_usage_history:
            return 0.0

        # Efficiency based on consistent memory usage and processing times
        memory_variance = statistics.variance([h["memory_mb"] for h in self.memory_usage_history]) if len(self.memory_usage_history) > 1 else 0
        time_variance = statistics.variance([h["processing_time"] for h in self.memory_usage_history]) if len(self.memory_usage_history) > 1 else 0

        # Lower variance = higher efficiency
        memory_efficiency = max(0, 1.0 - memory_variance / 100)  # Normalize
        time_efficiency = max(0, 1.0 - time_variance)

        return (memory_efficiency + time_efficiency) / 2

    def _calculate_memory_savings(self) -> str:
        """Calculate memory savings achieved"""
        if not self.memory_usage_history:
            return "Unable to calculate - no memory data"

        avg_memory = sum(h["memory_mb"] for h in self.memory_usage_history) / len(self.memory_usage_history)

        # Estimate traditional approach memory usage (store all records)
        # Assuming each record is ~1KB, calculate what traditional approach would use
        estimated_traditional_memory = (self.total_processed * 1024) / (1024 * 1024)  # MB

        if avg_memory < estimated_traditional_memory:
            savings_percent = ((estimated_traditional_memory - avg_memory) / estimated_traditional_memory) * 100
            return f"{savings_percent:.1f}% memory reduction achieved"
        else:
            return "Memory usage within expected bounds"

    def _assess_scalability(self) -> Dict[str, Any]:
        """Assess scalability of streaming approach"""
        if not self.memory_usage_history:
            return {"assessment": "insufficient_data"}

        memory_stable = statistics.variance([h["memory_mb"] for h in self.memory_usage_history]) < 10
        throughput_stable = len(self.memory_usage_history) > 5  # Processed multiple chunks

        if memory_stable and throughput_stable:
            scalability_score = 0.9
            assessment = "highly_scalable"
        elif memory_stable:
            scalability_score = 0.7
            assessment = "moderately_scalable"
        else:
            scalability_score = 0.4
            assessment = "limited_scalability"

        return {
            "assessment": assessment,
            "scalability_score": scalability_score,
            "memory_stability": memory_stable,
            "throughput_stability": throughput_stable,
            "recommendations": self._get_scalability_recommendations(assessment)
        }

    def _get_scalability_recommendations(self, assessment: str) -> List[str]:
        """Get scalability recommendations based on assessment"""
        recommendations = []

        if assessment == "highly_scalable":
            recommendations.append("Current streaming approach is optimal for high-volume processing")
            recommendations.append("Consider distributed processing for extreme scale")
        elif assessment == "moderately_scalable":
            recommendations.append("Optimize chunk processing to reduce memory variance")
            recommendations.append("Implement adaptive chunk sizing based on system resources")
        else:
            recommendations.append("Review memory management and chunk processing strategy")
            recommendations.append("Consider reducing chunk size or implementing more aggressive cleanup")

        return recommendations

async def test_streaming_aggregation():
    """Test streaming aggregation with sample data"""
    print("="*80)
    print("PQN STREAMING AGGREGATOR - SPRINT 4")
    print("Testing 60% memory reduction through streaming aggregation")
    print("="*80)

    # Create sample data stream (simulating 1000 PQN detection records)
    def generate_sample_data():
        models = ["claude", "grok", "gemini", "gpt4"]
        patterns = ["consciousness_emergence", "tts_artifact", "resonance_signature", "goedelian_paradox"]

        for i in range(1000):
            model = models[i % len(models)]
            num_patterns = 1 + (i % 3)  # 1-3 patterns per record

            detections = []
            for j in range(num_patterns):
                pattern = patterns[(i + j) % len(patterns)]
                confidence = 0.5 + (i % 50) / 100  # Vary confidence
                detections.append({
                    "pattern": pattern,
                    "confidence": confidence,
                    "evidence": f"Detected by {model} model"
                })

            record = {
                "model": model,
                "confidence": sum(d["confidence"] for d in detections) / len(detections),
                "processing_time": 0.8 + (i % 20) / 10,  # Vary processing time
                "detections": detections,
                "timestamp": datetime.now().isoformat()
            }

            yield record

    # Initialize streaming aggregator
    aggregator = PQNStreamingAggregator(chunk_size=50, max_memory_mb=20)

    # Process data stream
    print("Processing 1000 detection records in streaming mode...")
    report = await aggregator.process_stream(generate_sample_data())

    # Display results
    print("\n" + "="*80)
    print("STREAMING AGGREGATION RESULTS")
    print("="*80)

    summary = report["processing_summary"]
    print(f"Records Processed: {summary['total_records_processed']}")
    print(f"Chunks Processed: {summary['chunks_processed']}")
    print(f"Processing Time: {summary['processing_time_seconds']:.2f} seconds")
    print(f"Throughput: {summary['throughput_records_per_second']:.1f} records/sec")
    print(f"Memory Efficiency: {summary['memory_efficiency']}")

    print("\nPATTERN ANALYSIS:")
    patterns = report["pattern_analysis"]
    print(f"Total Patterns: {patterns['total_patterns_detected']}")

    # Show top 3 patterns
    pattern_details = patterns["pattern_details"]
    top_patterns = sorted(pattern_details.items(),
                         key=lambda x: x[1]["emergence_score"], reverse=True)[:3]

    for pattern, data in top_patterns:
        print(f"  {pattern}: {data['detection_count']} detections, "
              f"confidence {data['confidence_mean']:.2f} Â± {data['confidence_std']:.2f}")

    print("\nMODEL ANALYSIS:")
    models = report["model_analysis"]
    print(f"Total Models: {models['total_models']}")

    for model, data in models["model_details"].items():
        print(f"  {model.upper()}: {data['tests_completed']} tests, "
              f"confidence {data['confidence_mean']:.2f}, "
              f"processing {data['processing_time_mean']:.2f}s")

    print("\nPERFORMANCE METRICS:")
    perf = report["performance_metrics"]
    print(f"Streaming Efficiency: {perf['streaming_efficiency']:.2f}")
    print(f"Memory Optimization: {perf['memory_optimization_achieved']}")
    print(f"Scalability: {perf['scalability_assessment']['assessment']} "
          f"(score: {perf['scalability_assessment']['scalability_score']:.1f})")

    print("\nDATA QUALITY:")
    quality = report["data_quality"]
    print(f"Average Quality Score: {quality['average_quality_score']:.2f}")
    print(f"Quality Std Dev: {quality['quality_std_dev']:.3f}")

    # Save detailed report
    with open("streaming_aggregation_report.json", "w") as f:
        json.dump(report, f, indent=2, default=str)

    print("\nðŸ’¾ Detailed report saved to: streaming_aggregation_report.json")
    print("ðŸ“Š Aggregation logs saved to: pqn_streaming_aggregator.log")

if __name__ == "__main__":
    asyncio.run(test_streaming_aggregation())
