"""
YouTube Shorts Scheduler - Main Orchestrator

WSP 80 DAE pattern for automated Shorts scheduling.
Connects to Chrome (9222) or Edge (9223) debug sessions.
"""

import asyncio
import logging
import os
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.edge.options import Options as EdgeOptions
from selenium.common.exceptions import WebDriverException

from .channel_config import get_channel_config, CHANNELS
from .dom_automation import YouTubeStudioDOM
from .schedule_tracker import ScheduleTracker
from .schedule_dba import record_schedule_outcome
from .index_weave import (
    ensure_index_json,
    load_index_json,
    save_index_json,
    build_topic_hashtags,
    build_human_description_context,
    inject_context_into_description,
    build_digital_twin_index_block,
    weave_description,
    update_index_after_schedule,
)
from .content_generator import (
    generate_clickbait_title,
    generate_clickbait_title_from_index,
    get_standard_description,
    generate_description_with_context,
)

logger = logging.getLogger(__name__)


class YouTubeShortsScheduler:
    """
    Main orchestrator for YouTube Shorts scheduling automation.

    Supports:
    - Move2Japan (Chrome 9222)
    - UnDaoDu (Chrome 9222)
    - FoundUps (Edge 9223)
    - RavingANTIFA (Edge 9223)

    Usage:
        scheduler = YouTubeShortsScheduler("move2japan")
        await scheduler.run_scheduling_cycle(max_videos=10)
    """

    def __init__(
        self,
        channel_key: str,
        storage_dir: Optional[Path] = None,
        dry_run: bool = False,
    ):
        """
        Initialize scheduler for a channel.

        Args:
            channel_key: "move2japan", "undaodu", "foundups", or "ravingantifa"
            storage_dir: Optional custom storage directory
            dry_run: If True, don't actually make changes
        """
        self.channel_key = channel_key.lower()
        self.config = get_channel_config(self.channel_key)

        if not self.config:
            raise ValueError(f"Unknown channel: {channel_key}")

        self.channel_id = self.config["id"]
        self.channel_name = self.config["name"]
        self.chrome_port = self.config["chrome_port"]
        self.time_slots = self.config["time_slots"]
        self.max_per_day = self.config["max_per_day"]
        self.dry_run = dry_run

        # Initialize tracker
        self.tracker = ScheduleTracker(self.channel_id, storage_dir)

        # Driver and DOM (initialized on connect)
        self.driver = None
        self.dom = None

        logger.info(f"[SCHEDULER] Initialized for {self.channel_name} (port {self.chrome_port})")

    # =========================================
    # BROWSER CONNECTION
    # =========================================

    def connect_browser(self) -> bool:
        """
        Connect to existing Chrome/Edge debug session.

        Returns:
            True if connected successfully
        """
        try:
            if self.chrome_port == 9223:
                # Edge browser for FoundUps
                options = EdgeOptions()
                options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.chrome_port}")
                self.driver = webdriver.Edge(options=options)
                logger.info(f"[SCHEDULER] Connected to Edge on port {self.chrome_port}")
            else:
                # Chrome browser for Move2Japan/UnDaoDu
                options = ChromeOptions()
                options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.chrome_port}")
                self.driver = webdriver.Chrome(options=options)
                logger.info(f"[SCHEDULER] Connected to Chrome on port {self.chrome_port}")

            # Initialize DOM automation layer
            self.dom = YouTubeStudioDOM(self.driver)
            return True

        except WebDriverException as e:
            logger.error(f"[SCHEDULER] Failed to connect: {e}")
            return False

    def disconnect(self):
        """Disconnect from browser (doesn't close it)."""
        if self.driver:
            try:
                # Don't quit - just disconnect
                self.driver = None
                self.dom = None
                logger.info("[SCHEDULER] Disconnected from browser")
            except Exception as e:
                logger.warning(f"[SCHEDULER] Disconnect error: {e}")

    def reconnect_browser(self, max_retries: int = 3) -> bool:
        """
        Reconnect to browser with retry logic.

        Used when WebDriver connection becomes stale or crashes.

        Args:
            max_retries: Maximum reconnection attempts

        Returns:
            True if reconnected successfully
        """
        import time

        for attempt in range(max_retries):
            logger.info(f"[SCHEDULER] Reconnection attempt {attempt + 1}/{max_retries}")

            # Clean up existing connection
            self.disconnect()
            time.sleep(1)  # Brief pause before reconnect

            # Try to connect
            if self.connect_browser():
                # Verify connection is healthy
                if self.dom and self.dom.check_driver_health(thorough=True):
                    logger.info("[SCHEDULER] Reconnection successful")
                    return True
                else:
                    logger.warning("[SCHEDULER] Reconnected but health check failed")

            time.sleep(2 * (attempt + 1))  # Exponential backoff

        logger.error(f"[SCHEDULER] Failed to reconnect after {max_retries} attempts")
        return False

    def ensure_healthy_connection(self) -> bool:
        """
        Ensure WebDriver connection is healthy, reconnect if needed.

        Returns:
            True if connection is healthy (or reconnected successfully)
        """
        if not self.driver or not self.dom:
            logger.warning("[SCHEDULER] No driver/DOM - attempting reconnect")
            return self.reconnect_browser()

        # Check health
        if not self.dom.check_driver_health(thorough=True):
            logger.warning("[SCHEDULER] Driver unhealthy - attempting reconnect")
            return self.reconnect_browser()

        return True

    # =========================================
    # SCHEDULING WORKFLOW
    # =========================================

    async def run_scheduling_cycle(
        self,
        max_videos: int = 0,
        update_metadata: bool = True,
    ) -> Dict[str, Any]:
        """
        Run a full scheduling cycle.

        1. Navigate to unlisted Shorts
        2. For each video:
           a. Update title/description if needed
           b. Schedule to next available slot
        3. Track all scheduled videos

        Args:
            max_videos: Maximum videos to process (0 = unlimited, process all)
            update_metadata: Whether to update titles/descriptions

        Returns:
            Summary dict with scheduled_count, errors, etc.
        """
        if not self.driver:
            raise RuntimeError("Not connected to browser. Call connect_browser() first.")

        # Ensure healthy connection before starting (reconnect if stale)
        if not self.ensure_healthy_connection():
            raise RuntimeError("WebDriver connection is unhealthy and reconnection failed")

        import time as _time
        cycle_start = _time.time()

        results = {
            "channel": self.channel_name,
            "channel_id": self.channel_id,
            "started_at": datetime.now().isoformat(),
            "scheduled": [],
            "errors": [],
            "skipped": [],
        }

        # Pre-cycle schedule report
        self.tracker.log_schedule_report()

        try:
            # Step 1: Navigate to unlisted Shorts (with fallback for robustness)
            logger.info(f"[SCHEDULER] Navigating to unlisted Shorts for {self.channel_name}")
            if not self.dom.navigate_to_shorts_with_fallback(self.channel_id, "UNLISTED"):
                logger.error("[SCHEDULER] Failed to navigate to unlisted Shorts")
                results["errors"].append({"error": "Navigation to unlisted Shorts failed"})
                return results
            await asyncio.sleep(2)  # Wait for page load

            # Step 1.5: Set page size to 50 for larger batches (2026-01-28)
            self.dom.set_page_size(50)
            await asyncio.sleep(1)

            # Step 2-4: CONTINUOUS PROCESSING LOOP (2026-01-28: Added for true "until complete")
            # Process batches of videos until no unlisted remain
            batch_num = 0
            total_processed = 0
            import os
            sync_enabled = os.getenv("YT_SCHEDULER_DO_SYNC", "false").lower() in ("1", "true", "yes")

            while True:
                batch_num += 1
                logger.info(f"[SCHEDULER] === BATCH {batch_num} ===")

                # Step 2: Get unlisted videos for this batch
                unlisted = self.dom.get_unlisted_videos()
                logger.info(f"[SCHEDULER] Found {len(unlisted)} unlisted videos in batch {batch_num}")

                if not unlisted:
                    if batch_num == 1:
                        results["message"] = "No unlisted videos found"
                    else:
                        results["message"] = f"All unlisted videos processed after {batch_num - 1} batches"
                    logger.info(f"[SCHEDULER] No more unlisted videos - continuous processing complete")
                    break

                # Step 3: Sync existing scheduled videos (disabled by default)
                if batch_num == 1:  # Only sync on first batch
                    await self._sync_scheduled_videos()
                    if sync_enabled:
                        logger.info("[SCHEDULER] Returning to unlisted Shorts after sync...")
                        if not self.dom.navigate_to_shorts_with_fallback(self.channel_id, "UNLISTED"):
                            logger.warning("[SCHEDULER] Could not return to unlisted Shorts, continuing with cached list")
                        await asyncio.sleep(2)
                        # Re-fetch after returning
                        unlisted = self.dom.get_unlisted_videos()

                # Step 4: Process each video in this batch
                processed = 0
                slots_exhausted = False  # 2026-01-29: Track if slots ran out (fixes infinite loop bug)
                # max_videos=0 means unlimited (process all)
                videos_to_process = unlisted if max_videos == 0 else unlisted[:max_videos]
                total_to_process = len(videos_to_process)
                logger.info(f"[SCHEDULER] Processing {total_to_process} videos (max_videos={max_videos}, batch={batch_num})")

                for video in videos_to_process:
                    if max_videos > 0 and processed >= max_videos:
                        break

                    video_id = video.get("video_id")
                    original_title = video.get("title", "")

                    # RESUME CAPABILITY: Skip already-scheduled videos
                    if video_id and self.tracker.is_video_scheduled(video_id):
                        logger.info(f"[SCHEDULER] Skipping (already scheduled): {video_id}")
                        results["skipped"].append({
                            "video_id": video_id,
                            "reason": "Already scheduled",
                        })
                        continue

                    import time as time_module
                    video_start = time_module.time()
                    logger.info(
                        f"[SCHEDULER] ▶ Video [{processed+1}/{total_to_process}] "
                        f"batch={batch_num} | {video_id} | {original_title[:50]}"
                    )

                    try:
                        # Get next available slot
                        slot = self.tracker.get_next_available_slot(
                            self.time_slots,
                            self.max_per_day,
                        )

                        if not slot:
                            logger.warning("[SCHEDULER] No available slots in date range - slots exhausted")
                            results["skipped"].append({
                                "video_id": video_id,
                                "reason": "No available slots",
                            })
                            slots_exhausted = True  # 2026-01-29: Signal outer loop to exit
                            break

                        date_str, time_str = slot

                        if self.dry_run:
                            logger.info(f"[DRY RUN] Would schedule {video_id} for {date_str} at {time_str}")
                            results["scheduled"].append({
                                "video_id": video_id,
                                "date": date_str,
                                "time": time_str,
                                "dry_run": True,
                            })
                            self.tracker.increment(date_str, video_id)
                            processed += 1
                            total_processed += 1
                            continue

                        # Navigate to video edit page
                        self.dom.navigate_to_video(video_id)
                        await asyncio.sleep(1.5)

                        # Update metadata if requested
                        if update_metadata:
                            await self._update_video_metadata(
                                video_id=video_id,
                                original_title=original_title,
                                date_str=date_str,
                                time_str=time_str,
                            )

                        # Schedule the video
                        success = self.dom.schedule_video(date_str, time_str)

                        if success:
                            self.tracker.increment(date_str, video_id)

                            # Update local index JSON with scheduling + description sync (best-effort)
                            try:
                                idx = load_index_json(channel_key=self.channel_key, video_id=video_id)
                                if isinstance(idx, dict):
                                    block = build_digital_twin_index_block(
                                        channel_key=self.channel_key,
                                        video_id=video_id,
                                        index_json=idx,
                                    )
                                    idx2 = update_index_after_schedule(
                                        index_json=idx,
                                        channel_key=self.channel_key,
                                        video_id=video_id,
                                        date_str=date_str,
                                        time_str=time_str,
                                        scheduled_by="0102",
                                        description_index_block=block,
                                    )
                                    save_index_json(channel_key=self.channel_key, video_id=video_id, data=idx2)
                            except Exception as exc:
                                logger.debug("[SCHEDULER] Index JSON update skipped: %s", exc)

                            # Record into DBA (PatternMemory) for 0102 recall.
                            record_schedule_outcome(
                                channel_id=self.channel_id,
                                video_id=video_id,
                                date_str=date_str,
                                time_str=time_str,
                                mode="schedule",
                                success=True,
                                agent="selenium",
                                details={"channel_key": self.channel_key},
                            )
                            video_elapsed = time_module.time() - video_start
                            n_done = len(results["scheduled"]) + 1
                            results["scheduled"].append({
                                "video_id": video_id,
                                "date": date_str,
                                "time": time_str,
                                "elapsed_sec": round(video_elapsed, 1),
                            })
                            logger.info(
                                f"[SCHEDULER] ✅ #{n_done} SCHEDULED: {video_id} → "
                                f"{date_str} @ {time_str} ({video_elapsed:.1f}s)"
                            )
                        else:
                            record_schedule_outcome(
                                channel_id=self.channel_id,
                                video_id=video_id,
                                date_str=date_str,
                                time_str=time_str,
                                mode="schedule",
                                success=False,
                                agent="selenium",
                                details={"channel_key": self.channel_key, "error": "Schedule failed"},
                            )
                            results["errors"].append({
                                "video_id": video_id,
                                "error": "Schedule failed",
                            })

                        processed += 1
                        total_processed += 1

                        # Human-like delay between videos
                        await asyncio.sleep(self.dom.human_delay(3.0, 1.0))

                    except Exception as e:
                        logger.error(f"[SCHEDULER] Error processing {video_id}: {e}")
                        results["errors"].append({
                            "video_id": video_id,
                            "error": str(e),
                        })

                # End of batch - navigate back to unlisted list for next batch (2026-01-28)
                logger.info(f"[SCHEDULER] Batch {batch_num} complete: {processed} videos processed")

                # 2026-01-31: Detect stale-unlisted videos (tracker says scheduled, YouTube says unlisted).
                # If ALL videos in a batch were skipped as "already scheduled" but YouTube still
                # shows them as unlisted, the prior scheduling action failed silently.
                # FIX: Purge false-positive IDs from tracker so the NEXT batch retries them.
                # Safety: only do this ONCE per cycle to prevent infinite purge-retry loops.
                batch_all_skipped = (processed == 0 and total_to_process > 0)
                if batch_all_skipped:
                    stale_ids = [
                        v.get("video_id") for v in videos_to_process
                        if v.get("video_id") and self.tracker.is_video_scheduled(v["video_id"])
                    ]
                    if stale_ids and not getattr(self, '_stale_purged', False):
                        logger.warning(
                            f"[SCHEDULER] All {total_to_process} videos in batch {batch_num} "
                            f"were already in tracker but STILL unlisted on YouTube — "
                            f"prior scheduling failed. Purging {len(stale_ids)} false positives "
                            f"from tracker: {stale_ids}"
                        )
                        # Purge each stale ID from the tracker so next iteration retries them
                        for stale_id in stale_ids:
                            self.tracker.remove_video(stale_id)
                        # Remove skipped entries for these IDs so they're re-processed
                        results["skipped"] = [
                            s for s in results["skipped"]
                            if s.get("video_id") not in stale_ids
                        ]
                        self._stale_purged = True  # One purge per cycle — prevent infinite loop
                        # DON'T break — let the loop retry these videos in the next batch
                        logger.info("[SCHEDULER] Tracker purged — retrying stale videos in next batch...")
                        # Navigate back so next iteration re-fetches
                    elif stale_ids and getattr(self, '_stale_purged', False):
                        # Already purged once this cycle, still stuck — break to avoid infinite loop
                        logger.error(
                            f"[SCHEDULER] Stale videos STILL failing after purge+retry: {stale_ids}. "
                            f"Breaking to prevent infinite loop."
                        )
                        results["message"] = (
                            f"Stopped: {len(stale_ids)} videos failed scheduling twice — "
                            f"possible DOM/YouTube issue"
                        )
                        break
                    else:
                        # All skipped but none are in tracker (shouldn't happen) — break safely
                        logger.warning(
                            f"[SCHEDULER] Batch {batch_num}: 0 processed, {total_to_process} videos, "
                            f"no stale IDs found — breaking"
                        )
                        break

                # 2026-01-29: Exit outer loop if slots exhausted (fixes infinite loop bug)
                if slots_exhausted:
                    results["message"] = f"All scheduling slots filled after {batch_num} batches ({total_processed} videos scheduled)"
                    logger.info(f"[SCHEDULER] Slots exhausted - stopping continuous processing")
                    break

                # If max_videos limit reached, stop
                if max_videos > 0 and total_processed >= max_videos:
                    logger.info(f"[SCHEDULER] Reached max_videos limit ({max_videos}), stopping")
                    break

                # Navigate back to unlisted Shorts list for next batch
                logger.info("[SCHEDULER] Navigating back to unlisted Shorts for next batch...")
                if not self.dom.navigate_to_shorts_with_fallback(self.channel_id, "UNLISTED"):
                    logger.warning("[SCHEDULER] Could not navigate back, attempting back button...")
                    self.dom.click_back_to_shorts_list()
                await asyncio.sleep(2)
                # Loop continues to get next batch

        except Exception as e:
            logger.error(f"[SCHEDULER] Cycle error: {e}")
            results["errors"].append({"error": str(e)})

        cycle_elapsed = _time.time() - cycle_start
        results["finished_at"] = datetime.now().isoformat()
        results["total_scheduled"] = len(results["scheduled"])
        results["total_errors"] = len(results["errors"])
        results["total_skipped"] = len(results["skipped"])
        results["cycle_seconds"] = round(cycle_elapsed, 1)

        # End-of-cycle report
        n_ok = results["total_scheduled"]
        n_err = results["total_errors"]
        n_skip = results["total_skipped"]
        logger.info(f"[SCHEDULER] ╔══ CYCLE COMPLETE: {self.channel_name} ══╗")
        logger.info(f"[SCHEDULER] ║ Scheduled: {n_ok:>4} videos")
        logger.info(f"[SCHEDULER] ║ Errors:    {n_err:>4}")
        logger.info(f"[SCHEDULER] ║ Skipped:   {n_skip:>4}")
        logger.info(f"[SCHEDULER] ║ Duration:  {cycle_elapsed:>7.1f}s")
        if n_ok > 0:
            avg = cycle_elapsed / n_ok
            logger.info(f"[SCHEDULER] ║ Avg/video: {avg:>7.1f}s")
            # Date spread
            dates_used = set(v["date"] for v in results["scheduled"])
            logger.info(f"[SCHEDULER] ║ Dates:     {len(dates_used):>4} unique days")
            for d in sorted(dates_used):
                vids_on_day = [v for v in results["scheduled"] if v["date"] == d]
                times = ", ".join(v["time"] for v in vids_on_day)
                logger.info(f"[SCHEDULER] ║   {d}: {times}")
        logger.info(f"[SCHEDULER] ╚{'═' * 42}╝")

        # Post-cycle schedule report (updated totals)
        self.tracker.log_schedule_report()

        # Post-cycle audit: verify scheduled state matches YouTube reality
        # Enable with YT_SCHEDULER_POST_AUDIT=true (default: false — opt-in)
        if os.getenv("YT_SCHEDULER_POST_AUDIT", "false").lower() in ("1", "true", "yes"):
            try:
                from .schedule_auditor import ScheduleAuditor
                auditor = ScheduleAuditor(self.channel_key, self.driver)
                auto_heal = os.getenv("YT_SCHEDULER_AUDIT_AUTO_HEAL", "true").lower() in ("1", "true", "yes")
                audit_report = auditor.run_audit(auto_heal=auto_heal)
                results["audit"] = {
                    "healthy": audit_report.get("healthy", False),
                    "false_positives": len(audit_report.get("false_positives", [])),
                    "time_collisions": len(audit_report.get("time_collisions", [])),
                    "healed": len(audit_report.get("healed", [])),
                }
                if not audit_report.get("healthy"):
                    logger.warning(
                        f"[SCHEDULER] Post-cycle audit found issues: "
                        f"{len(audit_report.get('false_positives', []))} false positives, "
                        f"{len(audit_report.get('time_collisions', []))} time collisions"
                    )
            except Exception as e:
                logger.debug(f"[SCHEDULER] Post-cycle audit skipped: {e}")

        return results

    async def _sync_scheduled_videos(self):
        """Sync tracker with actual YouTube scheduled videos.

        NOTE: "Scheduled" isn't a filterable visibility option in YouTube Studio.
        The local tracker with persistence is sufficient for resume capability.
        This sync is now opt-in only (default skip) since it was causing errors.

        Can be enabled with YT_SCHEDULER_DO_SYNC=true if SCHEDULED filter becomes available.
        """
        import os

        # Sync is disabled by default - "Scheduled" isn't a real filter option
        # Local tracker persistence is sufficient for resume capability
        if os.getenv("YT_SCHEDULER_DO_SYNC", "false").lower() not in ("1", "true", "yes"):
            logger.debug("[SCHEDULER] Sync skipped (local tracker sufficient for resume)")
            return

        try:
            # Quick timeout for sync navigation - don't block main scheduling
            logger.info("[SCHEDULER] Syncing with YouTube schedule (experimental)...")

            # Navigate to scheduled filter (with fallback for robustness)
            if not self.dom.navigate_to_shorts_with_fallback(self.channel_id, "SCHEDULED"):
                logger.debug("[SCHEDULER] SCHEDULED filter not available, using local tracker")
                return
            await asyncio.sleep(2)

            # Get all scheduled videos (paginate if needed)
            all_scheduled = []
            while True:
                scheduled = self.dom.get_scheduled_videos()
                all_scheduled.extend(scheduled)

                if self.dom.has_next_page():
                    self.dom.click_next_page()
                    await asyncio.sleep(1)
                else:
                    break

            # Sync to tracker
            self.tracker.sync_from_youtube(all_scheduled)
            logger.info(f"[SCHEDULER] Synced {len(all_scheduled)} scheduled videos")

        except Exception as e:
            logger.debug(f"[SCHEDULER] Sync skipped: {e}")

    async def _update_video_metadata(
        self,
        *,
        video_id: str,
        original_title: str,
        date_str: str,
        time_str: str,
    ):
        """Update video title and description (optionally weave index)."""
        try:
            # Generate new title
            new_title = generate_clickbait_title(original_title=original_title)

            # Generate description
            base_description = get_standard_description(
                self.config.get("description_template", "ffcpln")
            )

            new_description = base_description

            # Optional: weave Digital Twin index into description.
            # Default ON; disable with YT_SCHEDULER_INDEX_WEAVE_ENABLED=false
            if os.getenv("YT_SCHEDULER_INDEX_WEAVE_ENABLED", "true").lower() in ("1", "true", "yes"):
                # For NEW unlisted Shorts, default to a local stub index (no API calls).
                # Override to "gemini" when you explicitly want full indexing.
                # NOTE: Gemini API cannot access UNLISTED videos! Use "stub" for new uploads.
                index_mode = os.getenv("YT_SCHEDULER_INDEX_MODE", "stub").strip().lower() or "stub"
                ensure = ensure_index_json(
                    channel_key=self.channel_key,
                    video_id=video_id,
                    allow_indexing_if_missing=True,
                    index_to_holoindex=True,
                    mode=index_mode,
                    stub_title=new_title,
                    stub_base_description=base_description,
                )

                # FALLBACK: If Gemini fails (e.g., unlisted videos), retry with stub mode
                if not ensure.ok and index_mode == "gemini":
                    logger.warning(f"[SCHEDULER] Gemini indexing failed ({ensure.error}), falling back to stub")
                    ensure = ensure_index_json(
                        channel_key=self.channel_key,
                        video_id=video_id,
                        allow_indexing_if_missing=True,
                        index_to_holoindex=False,  # Skip HoloIndex for stub
                        mode="stub",
                        stub_title=new_title,
                        stub_base_description=base_description,
                    )

                if not ensure.ok:
                    logger.warning(f"[SCHEDULER] Index weaving skipped: {ensure.error}")

                if ensure.ok:
                    idx = load_index_json(channel_key=self.channel_key, video_id=video_id)
                    if isinstance(idx, dict):
                        # Optional: allow index artifact to inform the title (best-effort).
                        if os.getenv("YT_SCHEDULER_INDEX_INFORM_TITLE", "false").lower() in ("1", "true", "yes"):
                            try:
                                new_title = generate_clickbait_title_from_index(
                                    original_title=original_title,
                                    index_json=idx,
                                )
                            except Exception as exc:
                                logger.debug("[SCHEDULER] Index-informed title skipped: %s", exc)

                        # Use indexing to enhance the human-facing description.
                        # Default ON; disable with YT_SCHEDULER_INDEX_ENHANCE_DESCRIPTION=false
                        enhanced_base = base_description
                        if os.getenv("YT_SCHEDULER_INDEX_ENHANCE_DESCRIPTION", "true").lower() in ("1", "true", "yes"):
                            context = build_human_description_context(idx)
                            enhanced_base = inject_context_into_description(
                                base_description=base_description,
                                context_block=context,
                            )

                        tags = build_topic_hashtags(idx, max_tags=5)
                        block = build_digital_twin_index_block(
                            channel_key=self.channel_key,
                            video_id=video_id,
                            index_json=idx,
                        )
                        new_description = weave_description(
                            base_description=enhanced_base,
                            index_block=block,
                            extra_hashtags=tags,
                        )

            # Update via DOM
            self.dom.edit_title(new_title)
            await asyncio.sleep(0.5)

            self.dom.edit_description(new_description)
            await asyncio.sleep(0.5)

            logger.info(f"[SCHEDULER] Updated metadata: {new_title[:50]}...")

        except Exception as e:
            logger.warning(f"[SCHEDULER] Metadata update failed: {e}")

    # =========================================
    # UTILITY METHODS
    # =========================================

    def get_schedule_summary(self) -> Dict:
        """Get current schedule summary."""
        return self.tracker.get_summary()

    async def preview_slots(self, count: int = 10) -> List[Dict]:
        """
        Preview next available slots without scheduling.

        Args:
            count: Number of slots to preview

        Returns:
            List of {date, time, slot_number} dicts
        """
        slots = []
        temp_tracker = ScheduleTracker(self.channel_id)

        for i in range(count):
            slot = temp_tracker.get_next_available_slot(
                self.time_slots,
                self.max_per_day,
            )
            if slot:
                date_str, time_str = slot
                slots.append({
                    "date": date_str,
                    "time": time_str,
                    "slot_number": i + 1,
                })
                temp_tracker.increment(date_str)
            else:
                break

        return slots

    # =========================================
    # INDEXING-ONLY MODE (Occam's Razor)
    # =========================================

    async def run_indexing_cycle(
        self,
        max_videos: int = 50,
        video_type: str = "all",  # "shorts", "videos", "all"
        sort_oldest: bool = True,
    ) -> Dict[str, Any]:
        """
        Run indexing WITHOUT scheduling (Occam's Razor - same flow, skip schedule step).

        This treats old videos the same as unlisted shorts, but:
        - Navigates to all videos (not just unlisted shorts)
        - Sorts by oldest first
        - Updates metadata (title, description + Digital Twin index)
        - SKIPS the scheduling step
        - Saves the video

        Args:
            max_videos: Maximum videos to process
            video_type: "shorts", "videos", or "all"
            sort_oldest: If True, sort by oldest first

        Returns:
            Summary dict with indexed_count, errors, etc.
        """
        if not self.driver:
            raise RuntimeError("Not connected to browser. Call connect_browser() first.")

        # Ensure healthy connection before starting (reconnect if stale)
        if not self.ensure_healthy_connection():
            raise RuntimeError("WebDriver connection is unhealthy and reconnection failed")

        results = {
            "channel": self.channel_name,
            "mode": "index_only",
            "started_at": datetime.now().isoformat(),
            "indexed": [],
            "errors": [],
            "skipped": [],
        }

        try:
            # Step 1: Navigate to channel videos (Content page)
            logger.info(f"[INDEXER] Navigating to {video_type} for {self.channel_name}")

            # Use /videos/upload for all videos, /videos/short for shorts
            if video_type == "shorts":
                content_url = f"https://studio.youtube.com/channel/{self.channel_id}/videos/short"
            else:
                content_url = f"https://studio.youtube.com/channel/{self.channel_id}/videos/upload"

            self.driver.get(content_url)
            await asyncio.sleep(3)

            # Step 2: Sort by oldest if requested
            if sort_oldest:
                logger.info("[INDEXER] Sorting by oldest first...")
                try:
                    sorted_ok = self.driver.execute_script("""
                        const sortButtons = document.querySelectorAll(
                            'ytcp-dropdown-trigger, button[aria-label*="Sort"], #sort-menu-button'
                        );
                        for (const btn of sortButtons) {
                            if (btn.textContent.toLowerCase().includes('date') ||
                                btn.getAttribute('aria-label')?.toLowerCase().includes('sort')) {
                                btn.click();
                                return 'clicked';
                            }
                        }
                        return 'not_found';
                    """)
                    if sorted_ok == 'clicked':
                        await asyncio.sleep(1)
                        self.driver.execute_script("""
                            const items = document.querySelectorAll('tp-yt-paper-item, ytcp-text-menu-item');
                            for (const item of items) {
                                if (item.textContent.toLowerCase().includes('oldest')) {
                                    item.click();
                                    return true;
                                }
                            }
                            return false;
                        """)
                        await asyncio.sleep(2)
                        logger.info("[INDEXER] ✓ Sorted by oldest")
                except Exception as e:
                    logger.warning(f"[INDEXER] Sort failed (using default): {e}")

            # Step 3: Get video list
            videos = self.dom.get_unlisted_videos() if video_type == "shorts" else self._get_all_videos()
            logger.info(f"[INDEXER] Found {len(videos)} videos to index")

            if not videos:
                results["message"] = "No videos found"
                return results

            # Step 4: Process each video (index only, NO scheduling)
            processed = 0
            for video in videos[:max_videos]:
                if processed >= max_videos:
                    break

                video_id = video.get("video_id")
                original_title = video.get("title", "")

                logger.info(f"[INDEXER] Processing: {video_id} - {original_title[:40]}...")

                try:
                    if self.dry_run:
                        logger.info(f"[DRY RUN] Would index {video_id}")
                        results["indexed"].append({"video_id": video_id, "dry_run": True})
                        processed += 1
                        continue

                    # Navigate to video edit page
                    self.dom.navigate_to_video(video_id)
                    await asyncio.sleep(1.5)

                    # Update metadata (same as scheduling - creates index + weaves description)
                    await self._update_video_metadata(
                        video_id=video_id,
                        original_title=original_title,
                        date_str="",  # No schedule date
                        time_str="",  # No schedule time
                    )

                    # SKIP SCHEDULING - This is the key difference!
                    # Just save the video (description was already updated)
                    save_ok = self.dom.save_video()
                    await asyncio.sleep(1)

                    if save_ok:
                        # Update local index JSON
                        try:
                            idx = load_index_json(channel_key=self.channel_key, video_id=video_id)
                            if isinstance(idx, dict):
                                idx["indexed_at"] = datetime.now().isoformat()
                                idx["indexed_by"] = "0102_indexer"
                                save_index_json(channel_key=self.channel_key, video_id=video_id, data=idx)
                        except Exception as exc:
                            logger.debug(f"[INDEXER] Index save skipped: {exc}")

                        results["indexed"].append({
                            "video_id": video_id,
                            "title": original_title[:50],
                        })
                        logger.info(f"[INDEXER] ✅ Indexed: {video_id}")
                    else:
                        results["errors"].append({
                            "video_id": video_id,
                            "error": "Save failed",
                        })

                    processed += 1

                    # Return to list for next video
                    self.driver.get(content_url)
                    await asyncio.sleep(2)

                except Exception as e:
                    logger.error(f"[INDEXER] Error processing {video_id}: {e}")
                    results["errors"].append({"video_id": video_id, "error": str(e)})

            results["indexed_count"] = len(results["indexed"])
            results["error_count"] = len(results["errors"])
            results["completed_at"] = datetime.now().isoformat()

        except Exception as e:
            logger.error(f"[INDEXER] Cycle failed: {e}")
            results["fatal_error"] = str(e)

        return results

    def _get_all_videos(self) -> List[Dict]:
        """Get all videos from current content page (not just unlisted)."""
        videos = []
        try:
            rows = self.driver.find_elements("css selector", "ytcp-video-row")
            for row in rows:
                try:
                    link = row.find_element("css selector", "a[href*='/video/']")
                    href = link.get_attribute("href") or ""
                    title_el = row.find_element("css selector", "#video-title")
                    title = title_el.text if title_el else ""

                    import re
                    match = re.search(r'/video/([^/?]+)', href)
                    if match:
                        videos.append({
                            "video_id": match.group(1),
                            "title": title,
                        })
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[INDEXER] Failed to get videos: {e}")
        return videos


# =========================================
# DAE ENTRY POINT
# =========================================

async def run_scheduler_dae(
    channel_key: str,
    max_videos: int = 0,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """
    DAE entry point for YouTube Shorts scheduling.

    Args:
        channel_key: "move2japan", "undaodu", "foundups", or "ravingantifa"
        max_videos: Maximum videos to schedule (0 = unlimited)
        dry_run: Preview mode without actual changes

    Returns:
        Scheduling results dict
    """
    # Phase -2: Ensure Chrome/Edge is running (same as comment engagement)
    try:
        from modules.infrastructure.dependency_launcher.src.dae_dependencies import ensure_dependencies
        deps = await ensure_dependencies(require_lm_studio=False)
        if not deps.get('chrome') and not deps.get('edge'):
            logger.error("[SCHEDULER] Chrome/Edge not available after dependency check")
            return {
                "error": "Browser dependencies not available",
                "channel": channel_key,
            }
    except Exception as e:
        logger.warning(f"[SCHEDULER] Dependency launcher not available: {e}")
        # Continue anyway - might already be running

    scheduler = YouTubeShortsScheduler(channel_key, dry_run=dry_run)

    if not scheduler.connect_browser():
        return {
            "error": f"Failed to connect to browser for {channel_key}",
            "channel": channel_key,
        }

    try:
        results = await scheduler.run_scheduling_cycle(max_videos=max_videos)
        return results
    finally:
        scheduler.disconnect()



async def run_indexer_dae(
    channel_key: str,
    max_videos: int = 50,
    video_type: str = "all",  # "shorts", "videos", "all"
    sort_oldest: bool = True,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """
    DAE entry point for Video Indexing (Occam's Razor approach).

    Same flow as scheduling but SKIPS the schedule step.
    Weaves Digital Twin index into description for cloud memory.

    Args:
        channel_key: "move2japan", "undaodu", "foundups", or "ravingantifa"
        max_videos: Maximum videos to index
        video_type: "shorts", "videos", or "all"
        sort_oldest: If True, process oldest videos first
        dry_run: Preview mode without actual changes

    Returns:
        Indexing results dict

    WSP Compliance:
        WSP 60: Memory artifacts in memory/video_index/{channel}/
        WSP 73: Digital Twin block in description (cloud memory)
        WSP 80: DAE pattern for background indexing
    """
    scheduler = YouTubeShortsScheduler(channel_key, dry_run=dry_run)

    if not scheduler.connect_browser():
        return {
            "error": f"Failed to connect to browser for {channel_key}",
            "channel": channel_key,
        }

    try:
        results = await scheduler.run_indexing_cycle(
            max_videos=max_videos,
            video_type=video_type,
            sort_oldest=sort_oldest,
        )
        return results
    finally:
        scheduler.disconnect()

# =========================================
# CLI INTERFACE
# =========================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="YouTube Shorts Scheduler")
    parser.add_argument(
        "channel",
        choices=["move2japan", "undaodu", "foundups", "ravingantifa"],
        help="Channel to schedule for",
    )
    parser.add_argument(
        "--max",
        type=int,
        default=0,
        help="Maximum videos to schedule (0 = unlimited, process all)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview mode without changes",
    )
    parser.add_argument(
        "--preview-slots",
        type=int,
        default=0,
        help="Preview N available slots",
    )

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    if args.preview_slots > 0:
        # Preview mode
        scheduler = YouTubeShortsScheduler(args.channel, dry_run=True)
        slots = asyncio.run(scheduler.preview_slots(args.preview_slots))
        print(f"\nNext {len(slots)} available slots for {args.channel}:")
        for slot in slots:
            print(f"  {slot['slot_number']}. {slot['date']} at {slot['time']}")
    else:
        # Run scheduling
        results = asyncio.run(run_scheduler_dae(
            args.channel,
            max_videos=args.max,
            dry_run=args.dry_run,
        ))

        print(f"\n=== Scheduling Results for {results.get('channel', args.channel)} ===")
        print(f"Scheduled: {results.get('total_scheduled', 0)}")
        print(f"Errors: {results.get('total_errors', 0)}")

        if results.get("scheduled"):
            print("\nScheduled videos:")
            for v in results["scheduled"]:
                dry = " [DRY RUN]" if v.get("dry_run") else ""
                print(f"  - {v['video_id']}: {v['date']} at {v['time']}{dry}")

        if results.get("errors"):
            print("\nErrors:")
            for e in results["errors"]:
                print(f"  - {e}")
