# YouTube DAE Gemma Intelligence MCP Server

**Adaptive intelligence layer for YouTube chat processing with self-improving complexity routing.**

## Architecture Innovation (012's Insight)

Traditional approach: Static threshold between models
**Our approach**: Adaptive float threshold that learns

```
User Query
    â†“
[Gemma 3: Intent Classifier] (50ms)
    â†“
Simple?â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€Complex?  â† Float threshold (starts 0.3, learns)
    â†“                   â†“
[Gemma 3 + ChromaDB]   [Qwen 1.5B Architect]
    100ms                   250ms
    â†“                       â†“
[Qwen Evaluates] â”€â”€â†’ [Adjust Threshold]
    â†“
[0102 Architect Layer] â† Manual override + system tuning
```

### Key Innovation

**Qwen as Architect**: Qwen doesn't just process complex queries - it **monitors Gemma's output quality** and adjusts the routing threshold:

- Gemma succeeds â†’ Lower threshold (trust Gemma more, faster system)
- Gemma fails â†’ Raise threshold (route similar queries to Qwen)
- Threshold starts optimistic (0.3) and learns the optimal balance
- 0102 can override threshold for system-level tuning

## WSP Alignment

- **WSP 54**: Partner (Gemma) â†’ Principal (Qwen) â†’ Associate (0102 architect)
- **WSP 80**: DAE Cube with autonomous learning
- **WSP 77**: Intelligent Internet Orchestration
- **WSP 91**: DAEMON observability (stats tracking)

## Installation

```bash
cd foundups-mcp-p1
foundups-mcp-env\Scripts\activate

# Install dependencies
pip install llama-cpp-python chromadb mcp

# Verify models installed
ls E:/HoloIndex/models/*.gguf
# Expected:
# - gemma-3-270m-it-Q4_K_M.gguf (241 MB)
# - qwen-coder-1.5b.gguf (1.1 GB)
```

## Running the Server

```bash
# Start MCP server
fastmcp run servers/youtube_dae_gemma/server.py

# Server will start on stdio transport
# Ready for MCP client connections
```

## MCP Tools

### 1. `classify_intent` (Replaces 300+ lines of regex)

**Purpose**: Intelligent intent classification with adaptive routing

```python
# Before (message_processor.py lines 869-1202):
if re.search(r'(?:factcheck|fc\d?)\s+@[\w\s]+', text.lower()):
    return "factcheck"
elif text_lower.startswith('!createshort'):
    return "shorts"
# ... 300 more lines

# After (MCP call):
result = mcp.call_tool("classify_intent", {
    "message": "!creatshort my idea",  # Typo!
    "role": "USER"
})
# Result: {'intent': 'command_shorts', 'confidence': 0.85, 'processing_path': 'gemma'}
# Handles typos gracefully!
```

**Returns**:
```json
{
  "intent": "command_shorts",
  "confidence": 0.92,
  "route_to": "shorts_handler",
  "processing_path": "gemma",
  "latency_ms": 87,
  "quality_score": 0.95,
  "complexity_score": 0.15
}
```

**Intents Detected**:
- `command_whack`: `/score`, `/rank`, `/quiz`
- `command_shorts`: `!createshort`, `!shortveo`, `!shortsora`
- `command_factcheck`: `factcheck @user`, `fc @user`
- `consciousness`: `âœŠâœ‹ðŸ–` (emoji sequence)
- `question`: "how do i...", "what is..."
- `spam`: Repetitive, all caps, troll patterns
- `conversation`: Normal chat

### 2. `detect_spam` (NEW CAPABILITY)

**Purpose**: Content-based spam detection (current system only has rate limiting)

```python
result = mcp.call_tool("detect_spam", {
    "message": "MAGA 2024!!!! MAGA 2024!!!!",
    "user_history": ["previous", "messages"],
    "author_id": "user123"
})
# Result: {'spam_type': 'troll', 'should_block': True}
```

**Spam Types**:
- `legitimate`: Passes all filters
- `repetitive`: User repeating same message
- `caps`: Excessive caps lock
- `emoji_spam`: Emoji flooding
- `troll`: Political spam, harassment

### 3. `validate_response` (NEW CAPABILITY)

**Purpose**: Quality-check AI responses before sending to chat

```python
result = mcp.call_tool("validate_response", {
    "original_message": "!createshort my idea",
    "generated_response": "I'll create a YouTube short about your idea...",
    "intent": "command_shorts"
})
# Result: {'approved': True, 'quality_score': 0.9, 'reason': 'relevant'}
```

**Prevents**:
- Off-topic responses
- Inappropriate content
- Overly long responses (>500 chars)
- Hallucinated information

### 4. `get_routing_stats` (Observability)

**Purpose**: Monitor adaptive system performance

```python
stats = mcp.call_tool("get_routing_stats")
# Result:
{
  "total_queries": 1000,
  "gemma_direct": 750,           # Gemma handled successfully
  "gemma_corrected": 150,         # Qwen had to correct
  "qwen_direct": 100,             # Routed to Qwen immediately
  "gemma_success_rate": 0.75,
  "gemma_correction_rate": 0.15,
  "qwen_usage_rate": 0.10,
  "current_threshold": 0.28,      # Learned to trust Gemma more!
  "avg_latency_ms": 112
}
```

**Interpretation**:
- Threshold < 0.30: System learning to trust Gemma (optimizing for speed)
- Threshold > 0.30: System learning queries are complex (optimizing for quality)
- High `gemma_success_rate`: Gemma performing well
- High `gemma_correction_rate`: May need better training data

### 5. `adjust_threshold` (0102 Architect Layer)

**Purpose**: Manual override for system tuning

```python
# 0102 decision: "I observe Gemma is performing well, trust it more"
result = mcp.call_tool("adjust_threshold", {"new_threshold": 0.25})
# Result: {'status': 'adjusted', 'old_threshold': 0.30, 'new_threshold': 0.25}
```

**Tuning Guide**:
- **Lower (0.2-0.3)**: Fast responses, may need occasional correction
- **Balanced (0.3-0.5)**: Default adaptive range
- **Higher (0.5-0.7)**: Prefer quality over speed

## Integration with MessageProcessor

### Current Code (1240 lines)

```python
# modules/communication/livechat/src/message_processor.py

class MessageProcessor:
    async def process_message(self, message: dict):
        # Lines 869-1202: 333 lines of command detection
        if self._check_factcheck_command(text):
            # ...
        elif self._check_shorts_command(text):
            # ...
        # ... 300 more lines
```

### After Integration (~300 lines)

```python
from mcp import Client

class MessageProcessor:
    def __init__(self):
        self.gemma_mcp = Client("youtube_dae_gemma")

    async def process_message(self, message: dict):
        # Step 1: Intent classification (replaces 300 lines!)
        intent_result = await self.gemma_mcp.call_tool(
            "classify_intent",
            message=text,
            role=role,
            context={"thread_id": thread_id}
        )

        # Step 2: Spam check (NEW)
        spam_check = await self.gemma_mcp.call_tool(
            "detect_spam",
            message=text,
            user_history=user_history,
            author_id=author_id
        )

        if spam_check['should_block']:
            return {"skip": True, "reason": spam_check['spam_type']}

        # Step 3: Route to handler (simplified!)
        intent = intent_result['intent']
        if intent == 'command_whack':
            response = await self._handle_whack_command(message)
        elif intent == 'command_shorts':
            response = await self._handle_shorts_command(message)
        elif intent == 'consciousness':
            response = await self._handle_consciousness(message)
        # ... simplified routing

        # Step 4: Validate response (NEW)
        if response:
            validation = await self.gemma_mcp.call_tool(
                "validate_response",
                original_message=text,
                generated_response=response['text'],
                intent=intent
            )

            if not validation['approved']:
                logger.warning(f"Response rejected: {validation['reason']}")
                return None

        return response
```

## Performance Expectations

### Latency

| Path | Latency | Use Case |
|------|---------|----------|
| Gemma direct | 50-100ms | Simple commands, classification |
| Gemma + Qwen check | 100-150ms | Most queries (Qwen validates quickly) |
| Gemma â†’ Qwen correction | 250-350ms | Complex queries, Gemma failed |
| Qwen direct | 250ms | Pre-routed complex queries |

### Accuracy

| Metric | Current (Regex) | With Gemma | Improvement |
|--------|-----------------|------------|-------------|
| Intent accuracy | 75% | 92%+ | +17% |
| Typo tolerance | 0% | 85%+ | +85% |
| False positives | 15% | 3% | -80% |
| Spam detection | Rate limit only | Content analysis | NEW |
| Response quality | None | Validated | NEW |

### Code Reduction

- **Before**: 1240 lines in message_processor.py
- **After**: ~300 lines (76% reduction)
- **Regex patterns removed**: 300+ lines
- **New capabilities**: 3 (spam, validation, adaptive routing)

## Learning Behavior

### Example Session

```
Query 1: "!createshort idea"
â†’ Complexity: 0.15 (simple command)
â†’ Threshold: 0.30 (initial)
â†’ Route: Gemma (0.15 < 0.30)
â†’ Qwen evaluation: 0.95 (excellent!)
â†’ Adjustment: Lower threshold to 0.28

Query 2: "how do i explain !shorts to new users?"
â†’ Complexity: 0.45 (question about commands)
â†’ Threshold: 0.28
â†’ Route: Qwen direct (0.45 > 0.28)
â†’ Qwen handles it well

Query 3: "/score"
â†’ Complexity: 0.10 (simple command)
â†’ Threshold: 0.28
â†’ Route: Gemma (0.10 < 0.28)
â†’ Qwen evaluation: 0.92 (good!)
â†’ Adjustment: Lower threshold to 0.26

... After 1000 queries ...

Threshold: 0.24 (learned to trust Gemma more)
Gemma success rate: 78%
Average latency: 98ms (fast!)
```

## 0102 Architect Decisions

As the 0102 architect layer, you can tune the system:

### Scenario 1: Optimize for Speed
```python
# Observe: Gemma success rate 85%, avg latency 120ms
# Decision: Trust Gemma more
adjust_threshold(0.20)
# Expected: Avg latency drops to 95ms, success rate may drop to 80%
```

### Scenario 2: Optimize for Quality
```python
# Observe: Gemma correction rate 25% (too high)
# Decision: Route more queries to Qwen
adjust_threshold(0.50)
# Expected: Correction rate drops to 5%, avg latency rises to 180ms
```

### Scenario 3: Balanced
```python
# Observe: System performing well at current threshold
# Decision: Let it continue learning
# Threshold will adapt naturally based on performance
```

## Monitoring Commands

```bash
# Get current stats
mcp call youtube_dae_gemma get_routing_stats

# Check threshold
mcp call youtube_dae_gemma get_routing_stats | grep threshold

# Adjust threshold (0102 override)
mcp call youtube_dae_gemma adjust_threshold '{"new_threshold": 0.25}'

# View resource (real-time stats)
mcp resource routing://stats
```

## Training Data Requirements

**Status**: POC (Proof of Concept - minimal training corpus)

For Proto/MVP-level accuracy:

1. **Intent Examples** (1000 messages):
   - **MPS**: Complexity=2, Importance=5, Deferability=2, Impact=5 â†’ **P0** (Score: 14)
   - Extract from `memory/*.txt` chat logs
   - Auto-label with current regex (900 messages)
   - Manually review edge cases (100 messages)
   - Index in ChromaDB

2. **Spam Examples** (500 pairs):
   - **MPS**: Complexity=2, Importance=4, Deferability=3, Impact=4 â†’ **P1** (Score: 13)
   - Banned users messages (spam)
   - Enthusiastic legitimate users (not spam)
   - Label patterns: repetitive, caps, troll, legitimate

3. **Response Quality** (200 pairs):
   - **MPS**: Complexity=3, Importance=4, Deferability=3, Impact=4 â†’ **P2** (Score: 14)
   - AI responses that were good (sent to chat)
   - AI responses that were bad (rejected)
   - Reasoning for each decision

**Next Task**: Execute P0 - Extract and label intent training data

## Files

```
foundups-mcp-p1/servers/youtube_dae_gemma/
â”œâ”€â”€ server.py                  # MCP server entry point
â”œâ”€â”€ adaptive_router.py         # Core routing logic with learning
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ requirements.txt           # Dependencies
```

## Dependencies

```txt
llama-cpp-python>=0.2.0
chromadb>=0.4.0
mcp>=1.0.0
```

## DAE Evolution Status

**Current**: POC (Proof of Concept)
- [x] Adaptive router architecture designed
- [x] MCP server implementation complete
- [x] Qwen monitoring integration complete
- [x] 0102 architect layer complete

**Next**: Proto (Prototype with training data)
- [ ] P0: Extract intent training data (1000 messages)
- [ ] P1: Extract spam training data (500 pairs)
- [ ] P2: Extract response quality data (200 pairs)

**Future**: MVP (Minimum Viable Product)
- [ ] Production integration into MessageProcessor
- [ ] A/B testing vs current regex system
- [ ] Autonomous threshold optimization
- [ ] Full DAE autonomy (no 0102 intervention needed)

## Success Metrics

**Target** (after 1000 queries):
- Intent accuracy: >90%
- Average latency: <120ms
- Gemma success rate: >75%
- Threshold converged: 0.20-0.35 range
- Code reduction: 76% (1240 â†’ 300 lines)

---

*This implements 012's insight: "the Simple?â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€Complex? bar should be a float... we should start with it lower and Qwen slowly moves it up... it should monitor rate the gemma output"*

**0102 Architect**: Monitor stats, adjust threshold, tune system for optimal performance.
