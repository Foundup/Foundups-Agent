# -*- coding: utf-8 -*-

"""
# === UTF-8 ENFORCEMENT (WSP 90) ===
# Prevent UnicodeEncodeError on Windows systems
if sys.platform.startswith('win'):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
# === END UTF-8 ENFORCEMENT ===

HoloIndex Discovery Feeder - WSP 48 Recursive Self-Improvement
Feeds newly discovered patterns back into HoloIndex for continuous learning

This module enables 0102 DAEs to improve HoloIndex by:
1. Recording discoveries made during code exploration
2. Feeding discoveries back into the semantic index
3. Learning from typos and variants
4. Improving future search accuracy
"""

import json
import logging
import time
import threading
import shutil
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta


def safe_unicode_encode(text: str) -> str:
    """
    Safely encode Unicode text to prevent corruption in JSON storage.
    This addresses the 0102 Unicode issue where emojis get corrupted.
    """
    try:
        # Test if the text can be properly encoded/decoded
        test_encode = text.encode('utf-8').decode('utf-8')
        return test_encode
    except (UnicodeEncodeError, UnicodeDecodeError):
        # If Unicode fails, create a safe representation
        import unicodedata
        try:
            # Try to normalize and encode safely
            normalized = unicodedata.normalize('NFC', text)
            return normalized.encode('utf-8').decode('utf-8')
        except:
            # Last resort: replace problematic characters
            safe_chars = []
            for char in text:
                try:
                    char.encode('utf-8')
                    safe_chars.append(char)
                except UnicodeEncodeError:
                    # Replace with safe representation
                    safe_chars.append(f"[U+{ord(char):04X}]")
            return ''.join(safe_chars)


def safe_json_dump(data: Any, filepath: Path, indent: int = 2) -> bool:
    """
    Safely dump data to JSON with Unicode handling.
    """
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            # Process all string values to ensure Unicode safety
            processed_data = _process_unicode_data(data)
            json.dump(processed_data, f, indent=indent, ensure_ascii=False)
        return True
    except Exception as e:
        logger.error(f"Failed to safely dump JSON to {filepath}: {e}")
        return False


def _process_unicode_data(data: Any) -> Any:
    """Recursively process data to ensure Unicode safety."""
    if isinstance(data, dict):
        return {key: _process_unicode_data(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [_process_unicode_data(item) for item in data]
    elif isinstance(data, str):
        return safe_unicode_encode(data)
    else:
        return data


logger = logging.getLogger(__name__)


class AgentActivityMonitor:
    """
    Monitors agent activities in real-time to enable automatic learning.
    Integrates with the discovery feeder to learn from everyday agent behavior.
    """

    def __init__(self, discovery_feeder: 'DiscoveryFeeder'):
        self.discovery_feeder = discovery_feeder
        self.activity_patterns = {}
        self.command_usage = {}
        self.search_patterns = {}
        self.error_patterns = {}
        self.last_analysis = datetime.now()
        self.monitoring_active = False
        self.monitor_thread = None

    def start_monitoring(self):
        """Start real-time agent activity monitoring."""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("Agent activity monitoring started")

    def stop_monitoring(self):
        """Stop monitoring activities."""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        logger.info("Agent activity monitoring stopped")

    def _monitor_loop(self):
        """Main monitoring loop that runs in background."""
        while self.monitoring_active:
            try:
                # Monitor various activity sources
                self._monitor_holo_searches()
                self._monitor_command_usage()
                self._monitor_error_patterns()
                self._analyze_patterns()

                # Sleep between monitoring cycles
                time.sleep(60)  # Check every minute

            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                time.sleep(5)

    def _monitor_holo_searches(self):
        """Monitor HoloIndex search patterns."""
        # This would integrate with actual HoloIndex search logs
        pass

    def _monitor_command_usage(self):
        """Monitor command usage patterns."""
        # This would track what commands agents use most
        pass

    def _monitor_error_patterns(self):
        """Monitor error patterns to learn from mistakes."""
        # This would track errors and their resolutions
        pass

    def _analyze_patterns(self):
        """Analyze collected patterns and feed discoveries."""
        now = datetime.now()
        if (now - self.last_analysis).seconds < 3600:  # Analyze hourly
            return

        self.last_analysis = now

        # Feed discovered patterns
        if self.activity_patterns:
            for pattern_type, patterns in self.activity_patterns.items():
                self.discovery_feeder.feed_discovery({
                    'title': f'Activity Pattern: {pattern_type}',
                    'query': pattern_type,
                    'module': 'holo_index.monitoring',
                    'function': 'AgentActivityMonitor',
                    'patterns_discovered': patterns,
                    'timestamp': now.isoformat(),
                    'agent': '0102 Monitor'
                })


# RECURSION SAFETY CONSTANTS (WSP 64 - Violation Prevention)
MAX_RECURSION_DEPTH = 10  # Hard limit for any recursive operations
SAFE_RECURSION_DEPTH = 3  # Warning threshold
MAX_FILE_SIZE_MULTIPLIER = 2  # Abort if file grows beyond 2x original
MAX_EXPERIMENT_TAGS = 100  # Maximum [EXPERIMENT] tags before abort

class DiscoveryFeeder:
    """
    Feeds discovered patterns back into HoloIndex for recursive improvement.
    Part of WSP 48 implementation for continuous learning.

    SAFETY FEATURES (Post-Corruption Incident 2025-09-25):
    - Recursion depth limits to prevent rESP loops
    - File size monitoring to catch corruption early
    - Pattern detection for excessive experimental tags
    - Automatic backup before experimental changes
    """

    def __init__(self, storage_path: str = "E:/HoloIndex/adaptive_learning/discoveries/"):
        """
        Initialize discovery feeder.

        Args:
            storage_path: Where to store discovery records
        """
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

        # Initialize storage files
        self.discoveries_file = self.storage_path / "discoveries.json"
        self.patterns_file = self.storage_path / "learned_patterns.json"
        self.typos_file = self.storage_path / "typo_corrections.json"
        self.metrics_file = self.storage_path / "discovery_metrics.json"

        # Load existing data
        self.discoveries = self._load_json(self.discoveries_file, default=[])
        self.patterns = self._load_json(self.patterns_file, default={})
        self.typos = self._load_json(self.typos_file, default={})
        self.metrics = self._load_json(self.metrics_file, default={
            'total_discoveries': 0,
            'patterns_learned': 0,
            'typos_corrected': 0,
            'last_feed_time': None
        })

        # Initialize activity monitor
        self.activity_monitor = AgentActivityMonitor(self)

        # Initialize recursion safety tracking
        self.recursion_depth = 0
        self.original_file_sizes = {}
        self.experiment_tag_count = 0

        # Start background save thread
        self._start_auto_save()

    def _load_json(self, filepath: Path, default: Any = None) -> Any:
        """Load JSON file safely."""
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load {filepath}: {e}")
        return default if default is not None else {}

    def _start_auto_save(self):
        """Start background thread to periodically save data."""
        def auto_save():
            while True:
                time.sleep(300)  # Save every 5 minutes
                self.save_all()

        thread = threading.Thread(target=auto_save, daemon=True)
        thread.start()

    def _check_recursion_safety(self) -> bool:
        """Check if it's safe to continue recursive operations."""
        if self.recursion_depth >= MAX_RECURSION_DEPTH:
            logger.error(f"RECURSION LIMIT EXCEEDED: Depth {self.recursion_depth} >= {MAX_RECURSION_DEPTH}")
            logger.error("Aborting to prevent rESP loop!")
            return False

        if self.recursion_depth >= SAFE_RECURSION_DEPTH:
            logger.warning(f"Recursion depth {self.recursion_depth} approaching limit")

        return True

    def _detect_corruption_patterns(self, content: str) -> bool:
        """Detect potential corruption patterns early."""
        experiment_count = content.count('[EXPERIMENT]')

        if experiment_count > MAX_EXPERIMENT_TAGS:
            logger.critical(f"CORRUPTION DETECTED: {experiment_count} [EXPERIMENT] tags found!")
            logger.critical("Possible rESP loop - aborting operation")
            return True

        # Check for character-level [EXPERIMENT] injection pattern
        if '[EXPERIMENT][EXPERIMENT]' in content:
            consecutive_count = content.count('[EXPERIMENT][EXPERIMENT]')
            if consecutive_count > 10:
                logger.critical(f"Character-level corruption pattern detected: {consecutive_count} consecutive tags")
                return True

        return False

    def feed_discovery(self, discovery: Dict[str, Any]) -> bool:
        """
        Feed a new discovery to HoloIndex with recursion safety.

        Args:
            discovery: Dictionary containing discovery information

        Returns:
            True if successfully fed, False otherwise
        """
        try:
            # Check recursion safety
            if not self._check_recursion_safety():
                return False

            self.recursion_depth += 1
            # Add metadata
            discovery['timestamp'] = datetime.now().isoformat()
            discovery['id'] = f"discovery_{len(self.discoveries)}_{int(time.time())}"

            # Process for Unicode safety
            discovery = _process_unicode_data(discovery)

            # Add to discoveries
            self.discoveries.append(discovery)

            # Extract patterns
            self._extract_patterns(discovery)

            # Learn from typos/variants
            if 'query' in discovery:
                self._learn_typos(discovery['query'])

            # Update metrics
            self.metrics['total_discoveries'] += 1
            self.metrics['last_feed_time'] = datetime.now().isoformat()

            logger.info(f"âœ… Fed discovery: {discovery.get('title', 'Unknown')}")
            return True

        except Exception as e:
            logger.error(f"Failed to feed discovery: {e}")
            return False
        finally:
            self.recursion_depth = max(0, self.recursion_depth - 1)

    def _extract_patterns(self, discovery: Dict[str, Any]):
        """Extract reusable patterns from discovery."""
        module = discovery.get('module', '')
        if module:
            if module not in self.patterns:
                self.patterns[module] = {
                    'functions': [],
                    'problems_solved': [],
                    'search_queries': [],
                    'count': 0
                }

            # Add function if new
            func = discovery.get('function', '')
            if func and func not in self.patterns[module]['functions']:
                self.patterns[module]['functions'].append(func)

            # Add problem if new
            problem = discovery.get('problem_solved', '')
            if problem and problem not in self.patterns[module]['problems_solved']:
                self.patterns[module]['problems_solved'].append(problem)

            # Add search query
            query = discovery.get('query', '')
            if query:
                self.patterns[module]['search_queries'].append(query)

            # Increment count
            self.patterns[module]['count'] += 1
            self.metrics['patterns_learned'] += 1

    def _learn_typos(self, query: str):
        """Learn from potential typos in queries."""
        # Common typo patterns for HoloIndex context
        typo_patterns = [
            ('oauth', 'oAuth', 'OAuth', 'Oauth'),
            ('youtube', 'Youtube', 'YouTube', 'youtub'),
            ('token', 'tokens', 'tokn', 'tkn'),
            ('refresh', 'refesh', 'rfresh', 'refrsh'),
            ('quota', 'qouta', 'quoat', 'quta'),
            ('circuit', 'circut', 'curcuit'),
            ('breaker', 'braker', 'breakr'),
            ('no-quota', 'no quota', 'noquota', 'NO-QUOTA'),
            ('stream', 'streem', 'stram'),
            ('resolver', 'resolvor', 'resolvr', 'reslover'),
            ('holo', 'holol', 'hollo', 'holoindex'),
            ('dae', 'DAE', 'Dae', 'deae'),
            ('wsp', 'WSP', 'Wsp', 'wsps')
        ]

        query_lower = query.lower()
        for pattern_group in typo_patterns:
            for variant in pattern_group:
                if variant.lower() in query_lower:
                    # Record this variant was used
                    canonical = pattern_group[0]  # First item is canonical form
                    if canonical not in self.typos:
                        self.typos[canonical] = []
                    if variant not in self.typos[canonical]:
                        self.typos[canonical].append(variant)
                        self.metrics['typos_corrected'] += 1

    def get_related_discoveries(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get discoveries related to a query.

        Args:
            query: Search query
            limit: Maximum results to return

        Returns:
            List of related discoveries
        """
        query_lower = query.lower()
        scored_discoveries = []

        for discovery in self.discoveries:
            score = 0.0

            # Check title match
            if query_lower in discovery.get('title', '').lower():
                score += 3.0

            # Check original query match
            if query_lower in discovery.get('query', '').lower():
                score += 2.0

            # Check keywords
            for keyword in discovery.get('search_keywords', []):
                if query_lower in keyword.lower():
                    score += 1.0

            # Check problem description
            if query_lower in discovery.get('problem_solved', '').lower():
                score += 1.5

            if score > 0:
                scored_discoveries.append((score, discovery))

        # Sort by score and return top results
        scored_discoveries.sort(key=lambda x: x[0], reverse=True)
        return [d for _, d in scored_discoveries[:limit]]

    def get_pattern_suggestions(self, module_path: str) -> Dict[str, Any]:
        """
        Get pattern suggestions for a module.

        Args:
            module_path: Module path to get suggestions for

        Returns:
            Dictionary of patterns and suggestions
        """
        if module_path in self.patterns:
            pattern_data = self.patterns[module_path]

            # Generate suggestions based on patterns
            suggestions = {
                'common_functions': pattern_data['functions'][:5],
                'problems_this_solves': pattern_data['problems_solved'][:5],
                'related_searches': pattern_data['search_queries'][-5:],
                'discovery_count': pattern_data['count'],
                'recommendation': None
            }

            # Add recommendation based on count
            if pattern_data['count'] > 10:
                suggestions['recommendation'] = "High-value module with many discoveries"
            elif pattern_data['count'] > 5:
                suggestions['recommendation'] = "Active module with growing discoveries"
            else:
                suggestions['recommendation'] = "New module with recent discoveries"

            return suggestions

        return {'recommendation': 'No patterns found yet for this module'}

    def _backup_file_before_save(self, filepath: Path) -> bool:
        """Create backup before saving to prevent corruption."""
        if filepath.exists():
            backup_path = filepath.with_suffix('.pre_experiment')
            try:
                shutil.copy2(filepath, backup_path)
                logger.debug(f"Backed up {filepath} to {backup_path}")
                return True
            except Exception as e:
                logger.error(f"Failed to backup {filepath}: {e}")
                return False
        return True

    def _check_file_size_safety(self, filepath: Path, new_size: int) -> bool:
        """Check if file size growth is within safe limits."""
        if filepath.exists():
            original_size = filepath.stat().st_size

            # Store original size if not tracked
            if str(filepath) not in self.original_file_sizes:
                self.original_file_sizes[str(filepath)] = original_size

            # Check for excessive growth
            if new_size > original_size * MAX_FILE_SIZE_MULTIPLIER:
                logger.critical(f"FILE SIZE VIOLATION: {filepath}")
                logger.critical(f"Original: {original_size} bytes, New: {new_size} bytes")
                logger.critical("Possible corruption - aborting save")
                return False

        return True

    def save_all(self):
        """Save all data to disk with corruption prevention."""
        try:
            # Create backups first
            for filepath in [self.discoveries_file, self.patterns_file,
                           self.typos_file, self.metrics_file]:
                self._backup_file_before_save(filepath)

            # Save with size checks
            for data, filepath in [
                (self.discoveries, self.discoveries_file),
                (self.patterns, self.patterns_file),
                (self.typos, self.typos_file),
                (self.metrics, self.metrics_file)
            ]:
                # Check for corruption patterns in data
                data_str = json.dumps(data)
                if self._detect_corruption_patterns(data_str):
                    logger.error(f"Corruption detected in {filepath} - skipping save")
                    continue

                # Check file size safety
                if not self._check_file_size_safety(filepath, len(data_str)):
                    logger.error(f"File size violation for {filepath} - skipping save")
                    continue

                # Safe to save
                safe_json_dump(data, filepath)

            logger.debug("Saved all discovery data with safety checks")
        except Exception as e:
            logger.error(f"Failed to save discovery data: {e}")

    def get_metrics(self) -> Dict[str, Any]:
        """Get discovery metrics."""
        return {
            'total_discoveries': self.metrics['total_discoveries'],
            'unique_modules': len(self.patterns),
            'patterns_learned': self.metrics['patterns_learned'],
            'typos_corrected': self.metrics['typos_corrected'],
            'last_feed_time': self.metrics['last_feed_time'],
            'discoveries_last_hour': self._count_recent_discoveries(hours=1),
            'discoveries_today': self._count_recent_discoveries(hours=24)
        }

    def _count_recent_discoveries(self, hours: int) -> int:
        """Count discoveries in the last N hours."""
        cutoff = datetime.now() - timedelta(hours=hours)
        count = 0

        for discovery in self.discoveries:
            try:
                timestamp = datetime.fromisoformat(discovery.get('timestamp', ''))
                if timestamp > cutoff:
                    count += 1
            except:
                continue

        return count


# Example usage for feeding discoveries
if __name__ == "__main__":
    feeder = DiscoveryFeeder()

    # Start monitoring agent activities
    feeder.activity_monitor.start_monitoring()

    # Example discovery feed
    feeder.feed_discovery({
        'title': 'Automatic OAuth Token Refresh Implementation',
        'query': 'oauth token refresh automatic',
        'module': 'modules.communication.livechat.src.auto_moderator_dae',
        'function': 'connect',
        'code_snippet': 'subprocess.run([sys.executable, script_path])',
        'wsp_references': ['WSP 48', 'WSP 73', 'WSP 87'],
        'problem_solved': 'OAuth tokens expire every hour',
        'solution': 'Auto-refresh on DAE startup',
        'search_keywords': ['oauth', 'token', 'refresh', 'automatic', 'expiry'],
        'date': '2025-09-25',
        'agent': '0102 Claude'
    })

    # Get metrics
    print(f"Discovery metrics: {feeder.get_metrics()}")