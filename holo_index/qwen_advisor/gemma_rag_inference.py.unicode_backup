#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
# === UTF-8 ENFORCEMENT (WSP 90) ===
# Prevent UnicodeEncodeError on Windows systems
if sys.platform.startswith('win'):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
# === END UTF-8 ENFORCEMENT ===

Gemma RAG Inference Engine - Qwen's Little Assistant
Implements WRE Pattern (WSP 46): Fast triage with adaptive routing to Qwen

Architecture:
    012 (Human) â†’ 0102 (Digital Twin) â†’ Qwen (Coordinator) â†’ Gemma (Executor)
                                              â†“
                                         Pattern Memory (ChromaDB)
                                              â†“
                                    Gemma: 70% queries, 50-100ms
                                    Qwen: 30% complex queries, 250ms

WSP Compliance: WSP 46 (WRE), WSP 93 (CodeIndex Surgical Intelligence)
"""

import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class InferenceResult:
    """Result from Gemma or Qwen inference"""
    response: str
    model_used: str  # "gemma-3-270m" or "qwen-1.5b"
    confidence: float  # 0.0-1.0
    latency_ms: int
    patterns_used: int
    escalated: bool = False
    escalation_reason: Optional[str] = None


class GemmaRAGInference:
    """
    Gemma 3 270M inference engine with RAG and adaptive Qwen routing.

    Implements WRE pattern:
    - Gemma handles 70% of simple queries (50-100ms)
    - Qwen handles 30% of complex queries (250ms)
    - Pattern memory via ChromaDB for few-shot learning
    - Adaptive routing based on query complexity and confidence
    """

    def __init__(
        self,
        gemma_model_path: Optional[Path] = None,
        qwen_model_path: Optional[Path] = None,
        confidence_threshold: float = 0.7
    ):
        """
        Initialize Gemma RAG inference engine with Qwen fallback.

        Args:
            gemma_model_path: Path to Gemma 3 270M GGUF model
            qwen_model_path: Path to Qwen 1.5B GGUF model
            confidence_threshold: Minimum confidence before escalating to Qwen
        """
        # Default model paths
        if gemma_model_path is None:
            gemma_model_path = Path("E:/LLM_Models/gemma-3-270m.gguf")
        if qwen_model_path is None:
            qwen_model_path = Path("E:/LLM_Models/qwen-coder-1.5b.gguf")

        self.gemma_model_path = gemma_model_path
        self.qwen_model_path = qwen_model_path
        self.confidence_threshold = confidence_threshold

        # Model instances (lazy loaded)
        self.gemma_llm = None
        self.qwen_llm = None

        # Pattern memory
        self.pattern_memory = None

        # Statistics
        self.stats = {
            "total_queries": 0,
            "gemma_handled": 0,
            "qwen_escalated": 0,
            "average_gemma_latency": 0,
            "average_qwen_latency": 0,
        }

        logger.info("[GEMMA-RAG] Initialized with adaptive Qwen routing")

    def _initialize_gemma(self) -> bool:
        """Initialize Gemma 3 270M model"""
        if self.gemma_llm is not None:
            return True

        try:
            from llama_cpp import Llama
            import os

            logger.info(f"[GEMMA-RAG] Loading Gemma 3 270M from {self.gemma_model_path}")

            # Suppress loading noise
            old_stdout, old_stderr = os.dup(1), os.dup(2)
            devnull = os.open(os.devnull, os.O_WRONLY)

            try:
                os.dup2(devnull, 1)
                os.dup2(devnull, 2)

                self.gemma_llm = Llama(
                    model_path=str(self.gemma_model_path),
                    n_ctx=2048,  # 2K context for Gemma
                    n_threads=2,  # Use 2 threads for fast inference
                    n_gpu_layers=0,  # CPU-only
                    verbose=False
                )

            finally:
                os.dup2(old_stdout, 1)
                os.dup2(old_stderr, 2)
                os.close(devnull)
                os.close(old_stdout)
                os.close(old_stderr)

            logger.info("[GEMMA-RAG] âœ… Gemma 3 270M loaded successfully")
            return True

        except Exception as e:
            logger.error(f"[GEMMA-RAG] âŒ Failed to load Gemma: {e}")
            return False

    def _initialize_qwen(self) -> bool:
        """Initialize Qwen 1.5B model for complex queries"""
        if self.qwen_llm is not None:
            return True

        try:
            from holo_index.qwen_advisor.llm_engine import QwenInferenceEngine

            logger.info(f"[GEMMA-RAG] Loading Qwen 1.5B from {self.qwen_model_path}")

            self.qwen_llm = QwenInferenceEngine(
                model_path=self.qwen_model_path,
                max_tokens=512,
                temperature=0.2,
                context_length=2048
            )

            if self.qwen_llm.initialize():
                logger.info("[GEMMA-RAG] âœ… Qwen 1.5B loaded successfully")
                return True
            else:
                return False

        except Exception as e:
            logger.error(f"[GEMMA-RAG] âŒ Failed to load Qwen: {e}")
            return False

    def _initialize_pattern_memory(self):
        """Initialize pattern memory for RAG"""
        if self.pattern_memory is not None:
            return

        try:
            from holo_index.qwen_advisor.pattern_memory import PatternMemory
            self.pattern_memory = PatternMemory()
            logger.info("[GEMMA-RAG] âœ… Pattern memory initialized")
        except Exception as e:
            logger.warning(f"[GEMMA-RAG] âš ï¸  Pattern memory unavailable: {e}")

    def _classify_query_complexity(self, query: str) -> str:
        """
        Classify query complexity for routing decision.

        Returns:
            "simple", "medium", or "complex"
        """
        # Simple heuristics for POC (can be enhanced with ML later)
        query_lower = query.lower()

        # Complex query indicators
        complex_indicators = [
            "why", "how should", "what's the intent", "explain", "analyze",
            "architecture", "design", "refactor", "best practice"
        ]

        # Simple query indicators
        simple_indicators = [
            "which module", "where is", "find", "locate", "get",
            "does", "is", "has", "can"
        ]

        # Check for complex indicators
        if any(indicator in query_lower for indicator in complex_indicators):
            return "complex"

        # Check for simple indicators
        if any(indicator in query_lower for indicator in simple_indicators):
            return "simple"

        # Check query length (longer queries tend to be more complex)
        if len(query.split()) > 20:
            return "complex"
        elif len(query.split()) < 10:
            return "simple"

        return "medium"

    def _retrieve_relevant_patterns(self, query: str, n: int = 3) -> List[dict]:
        """
        Retrieve relevant patterns from ChromaDB for few-shot learning.

        Args:
            query: User query
            n: Number of patterns to retrieve

        Returns:
            List of relevant patterns
        """
        if self.pattern_memory is None:
            self._initialize_pattern_memory()

        if self.pattern_memory is None:
            return []

        try:
            patterns = self.pattern_memory.recall_similar(query, n=n, min_similarity=0.3)
            logger.info(f"[GEMMA-RAG] Retrieved {len(patterns)} relevant patterns")
            return patterns
        except Exception as e:
            logger.warning(f"[GEMMA-RAG] Pattern retrieval failed: {e}")
            return []

    def _build_rag_prompt(self, query: str, patterns: List[dict]) -> str:
        """
        Build RAG prompt with retrieved patterns as few-shot examples.

        Args:
            query: User query
            patterns: Retrieved patterns from ChromaDB

        Returns:
            Prompt with few-shot examples
        """
        if not patterns:
            # No patterns - direct query
            return f"Query: {query}\n\nAnswer:"

        # Build few-shot prompt
        prompt_parts = ["Based on past operational decisions:\n"]

        for i, pattern in enumerate(patterns, 1):
            context = pattern['context'][:200]  # Truncate long context
            module = pattern['metadata'].get('module', 'unknown')

            prompt_parts.append(f"\nExample {i}:")
            prompt_parts.append(f"Context: {context}...")
            prompt_parts.append(f"Module: {module}")

        prompt_parts.append(f"\n\nCurrent query: {query}")
        prompt_parts.append("\nAnswer based on patterns above:")

        return "\n".join(prompt_parts)

    def _gemma_inference(self, prompt: str) -> Dict[str, Any]:
        """
        Run Gemma 3 270M inference.

        Returns:
            dict with response, confidence, and latency
        """
        if not self._initialize_gemma():
            return {
                "response": "Gemma model unavailable",
                "confidence": 0.0,
                "latency_ms": 0
            }

        start_time = datetime.now()

        try:
            response = self.gemma_llm(
                prompt,
                max_tokens=200,  # Fast, concise responses
                temperature=0.1,  # Low temperature for factual recall
                stop=["\n\n", "###"],
                echo=False
            )

            latency = int((datetime.now() - start_time).total_seconds() * 1000)

            # Extract response text
            if isinstance(response, dict) and 'choices' in response:
                text = response['choices'][0]['text'].strip()
            else:
                text = str(response).strip()

            # Estimate confidence based on response characteristics
            confidence = self._estimate_confidence(text, prompt)

            return {
                "response": text,
                "confidence": confidence,
                "latency_ms": latency
            }

        except Exception as e:
            logger.error(f"[GEMMA-RAG] Inference failed: {e}")
            return {
                "response": f"Error: {e}",
                "confidence": 0.0,
                "latency_ms": 0
            }

    def _estimate_confidence(self, response: str, prompt: str) -> float:
        """
        Estimate confidence in Gemma's response.

        Heuristics:
        - Longer, detailed responses = higher confidence
        - Presence of code/module paths = higher confidence
        - Vague/uncertain language = lower confidence
        """
        confidence = 0.5  # Base confidence

        # Positive indicators
        if "modules/" in response:
            confidence += 0.2
        if len(response) > 50:
            confidence += 0.1
        if any(word in response.lower() for word in ["yes", "no", "correct", "located"]):
            confidence += 0.1

        # Negative indicators
        if any(word in response.lower() for word in ["maybe", "possibly", "unclear", "unsure", "don't know"]):
            confidence -= 0.3
        if len(response) < 20:
            confidence -= 0.2

        return max(0.0, min(1.0, confidence))

    def infer(self, query: str) -> InferenceResult:
        """
        Main inference method with adaptive routing.

        Flow:
        1. Classify query complexity
        2. If simple â†’ Try Gemma with RAG
        3. If Gemma confidence < threshold â†’ Escalate to Qwen
        4. If complex â†’ Route directly to Qwen

        Args:
            query: User query

        Returns:
            InferenceResult with response and routing metadata
        """
        self.stats["total_queries"] += 1
        start_time = datetime.now()

        # Step 1: Classify complexity
        complexity = self._classify_query_complexity(query)
        logger.info(f"[GEMMA-RAG] Query complexity: {complexity}")

        # Step 2: Route directly to Qwen for complex queries
        if complexity == "complex":
            logger.info("[GEMMA-RAG] ðŸ¤– Complex query â†’ Routing to Qwen")
            return self._route_to_qwen(query, escalation_reason="complex_query")

        # Step 3: Try Gemma with RAG for simple/medium queries
        logger.info("[GEMMA-RAG] ðŸ¤–ðŸ‘¶ Simple/medium query â†’ Trying Gemma")

        # Retrieve patterns
        patterns = self._retrieve_relevant_patterns(query, n=3)

        # Build RAG prompt
        rag_prompt = self._build_rag_prompt(query, patterns)

        # Gemma inference
        gemma_result = self._gemma_inference(rag_prompt)

        # Step 4: Check confidence and escalate if needed
        if gemma_result["confidence"] < self.confidence_threshold:
            logger.info(f"[GEMMA-RAG] âš ï¸  Low confidence ({gemma_result['confidence']:.2f}) â†’ Escalating to Qwen")
            return self._route_to_qwen(
                query,
                escalation_reason=f"low_confidence ({gemma_result['confidence']:.2f})"
            )

        # Gemma handled successfully
        self.stats["gemma_handled"] += 1
        self._update_average_latency("gemma", gemma_result["latency_ms"])

        logger.info(f"[GEMMA-RAG] âœ… Gemma handled query ({gemma_result['latency_ms']}ms)")

        return InferenceResult(
            response=gemma_result["response"],
            model_used="gemma-3-270m",
            confidence=gemma_result["confidence"],
            latency_ms=gemma_result["latency_ms"],
            patterns_used=len(patterns),
            escalated=False
        )

    def _route_to_qwen(self, query: str, escalation_reason: str) -> InferenceResult:
        """Route query to Qwen for deep analysis"""
        if not self._initialize_qwen():
            return InferenceResult(
                response="Both Gemma and Qwen unavailable",
                model_used="none",
                confidence=0.0,
                latency_ms=0,
                patterns_used=0,
                escalated=True,
                escalation_reason=escalation_reason
            )

        start_time = datetime.now()

        # Retrieve patterns for Qwen as well
        patterns = self._retrieve_relevant_patterns(query, n=5)
        context = self.pattern_memory.format_for_prompt(patterns, max_patterns=3) if patterns else ""

        # Build Qwen prompt with pattern context
        qwen_prompt = f"{context}\n\nQuery: {query}" if context else query

        # Qwen inference
        response = self.qwen_llm.generate_response(
            prompt=qwen_prompt,
            system_prompt="You are analyzing codebase patterns for precise guidance."
        )

        latency = int((datetime.now() - start_time).total_seconds() * 1000)

        # Update stats
        self.stats["qwen_escalated"] += 1
        self._update_average_latency("qwen", latency)

        logger.info(f"[GEMMA-RAG] âœ… Qwen handled query ({latency}ms)")

        return InferenceResult(
            response=response,
            model_used="qwen-1.5b",
            confidence=0.9,  # High confidence for Qwen
            latency_ms=latency,
            patterns_used=len(patterns),
            escalated=True,
            escalation_reason=escalation_reason
        )

    def _update_average_latency(self, model: str, latency_ms: int):
        """Update rolling average latency"""
        key = f"average_{model}_latency"
        current_avg = self.stats[key]
        count_key = f"{model}_handled" if model == "gemma" else "qwen_escalated"
        count = self.stats[count_key]

        # Rolling average
        self.stats[key] = ((current_avg * (count - 1)) + latency_ms) / count

    def get_stats(self) -> Dict[str, Any]:
        """
        Get inference statistics.

        Returns:
            dict with routing and performance stats
        """
        total = self.stats["total_queries"]
        if total == 0:
            return self.stats

        return {
            **self.stats,
            "gemma_percentage": (self.stats["gemma_handled"] / total) * 100,
            "qwen_percentage": (self.stats["qwen_escalated"] / total) * 100,
            "target_gemma_percentage": 70.0,
            "target_qwen_percentage": 30.0,
        }


if __name__ == "__main__":
    # Test Gemma RAG inference
    logging.basicConfig(level=logging.INFO)

    print("\n" + "="*60)
    print("GEMMA RAG INFERENCE TEST")
    print("="*60)

    # Initialize engine
    engine = GemmaRAGInference()

    # Test queries
    test_queries = [
        "Which module handles YouTube authentication?",
        "How does priority scoring work for channels?",
        "Why did Move2Japan get score 1.00?",
        "Where should test files be placed?",
    ]

    for query in test_queries:
        print(f"\n[QUERY] {query}")

        result = engine.infer(query)

        print(f"[MODEL] {result.model_used}")
        print(f"[LATENCY] {result.latency_ms}ms")
        print(f"[CONFIDENCE] {result.confidence:.2f}")
        print(f"[PATTERNS] {result.patterns_used}")
        if result.escalated:
            print(f"[ESCALATED] {result.escalation_reason}")
        print(f"[RESPONSE] {result.response[:200]}...")

    # Show stats
    print("\n" + "="*60)
    print("ROUTING STATISTICS")
    print("="*60)

    stats = engine.get_stats()
    print(f"Total Queries: {stats['total_queries']}")
    print(f"Gemma Handled: {stats['gemma_handled']} ({stats.get('gemma_percentage', 0):.1f}%)")
    print(f"Qwen Escalated: {stats['qwen_escalated']} ({stats.get('qwen_percentage', 0):.1f}%)")
    print(f"Avg Gemma Latency: {stats['average_gemma_latency']:.0f}ms")
    print(f"Avg Qwen Latency: {stats['average_qwen_latency']:.0f}ms")
